{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c72f995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "import boto3\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split\n",
    "import requests\n",
    "from io import StringIO\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import sys\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb806c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', handlers=[logging.StreamHandler(sys.stdout)])\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6db3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = \"REDACTED_BUCKET\"\n",
    "dax_key = \"dax.csv\"\n",
    "def read_data_from_s3(bucket, key):\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "    return pd.read_csv(obj['Body'], parse_dates=['Date'])\n",
    "\n",
    "df = read_data_from_s3(bucket, dax_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a738a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0179ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabe2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7390b3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_column(data: pd.DataFrame, col, max_classes=None):\n",
    "    df = data.copy()\n",
    "    \n",
    "    if max_classes is not None:\n",
    "        dummies = pd.get_dummies(df[col], prefix=col, dtype=int)\n",
    "         \n",
    "        missing_cols = set(range(max_classes + 1)) - set(df[col].unique())\n",
    "        for col_num in missing_cols:\n",
    "            dummies[col +\"_\"+ str(col_num)] = 0\n",
    "        \n",
    "        # Custom sort function to ensure correct column order\n",
    "        dummies = dummies[sorted(dummies.columns, key=lambda x: int(x.split(\"_\")[1]))]\n",
    "    else:\n",
    "        dummies = pd.get_dummies(df[col], prefix=col, dtype=int)\n",
    "\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    if max_classes is None:\n",
    "        df = df.drop(columns=[col])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393d10d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Open_200_MA\"] = df[\"Open\"].rolling(window=200).mean()\n",
    "df[\"High_200_MA\"] = df[\"High\"].rolling(window=200).mean()\n",
    "df[\"Low_200_MA\"] = df[\"Low\"].rolling(window=200).mean()\n",
    "df[\"Close_200_MA\"] = df[\"Close\"].rolling(window=200).mean()\n",
    "df[\"Open_100_MA\"] = df[\"Open\"].rolling(window=100).mean()\n",
    "df[\"High_100_MA\"] = df[\"High\"].rolling(window=100).mean()\n",
    "df[\"Low_100_MA\"] = df[\"Low\"].rolling(window=100).mean()\n",
    "df[\"Close_100_MA\"] = df[\"Close\"].rolling(window=100).mean()\n",
    "df[\"Open_10_MA\"] = df[\"Open\"].rolling(window=10).mean()\n",
    "df[\"High_10_MA\"] = df[\"High\"].rolling(window=10).mean()\n",
    "df[\"Low_10_MA\"] = df[\"Low\"].rolling(window=10).mean()\n",
    "df[\"Close_10_MA\"] = df[\"Close\"].rolling(window=10).mean()\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2e7c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ab74ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df, x=\"Date\", y=[\"Open\",\"Open_200_MA\",\"Open_100_MA\",\"Open_10_MA\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3724baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ta\n",
    "\n",
    "\n",
    "# Calculate RSI\n",
    "df['RSI_open'] = ta.momentum.rsi(df['Open'])\n",
    "df['RSI_open'] = (df['RSI_open'] - 50) / 50\n",
    "df['RSI_diff_open'] = df['RSI_open'].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35642098",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df, x=\"Date\", y=['RSI_open'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030f3810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05e4cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Close_lag1'] = df['Close'].shift(1)\n",
    "df['Open_lag1'] = df['Open'].shift(1)\n",
    "df['Low_lag1'] = df['Low'].shift(1)\n",
    "df['High_lag1'] = df['High'].shift(1)\n",
    "\n",
    "df['Close_lag1_200_MA'] = df['Close_200_MA'].shift(1)\n",
    "df['Open_lag1_200_MA'] = df['Open_200_MA'].shift(1)\n",
    "df['Low_lag1_200_MA'] = df['Low_200_MA'].shift(1)\n",
    "df['High_lag1_200_MA'] = df['High_200_MA'].shift(1)\n",
    "\n",
    "df['Close_lag1_100_MA'] = df['Close_100_MA'].shift(1)\n",
    "df['Open_lag1_100_MA'] = df['Open_100_MA'].shift(1)\n",
    "df['Low_lag1_100_MA'] = df['Low_100_MA'].shift(1)\n",
    "df['High_lag1_100_MA'] = df['High_100_MA'].shift(1)\n",
    "\n",
    "df['Close_lag1_10_MA'] = df['Close_10_MA'].shift(1)\n",
    "df['Open_lag1_10_MA'] = df['Open_10_MA'].shift(1)\n",
    "df['Low_lag1_10_MA'] = df['Low_10_MA'].shift(1)\n",
    "df['High_lag1_10_MA'] = df['High_10_MA'].shift(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b60c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Open_Close\"] = (df[\"Open\"]- df[\"Close_lag1\"]) / df[\"Open\"]\n",
    "df[\"Open_Open\"] = (df[\"Open\"]- df[\"Open_lag1\"]) / df[\"Open\"]\n",
    "df[\"Open_Low\"] = (df[\"Open\"]- df[\"Low_lag1\"]) / df[\"Open\"]\n",
    "df[\"Open_High\"] = (df[\"Open\"]- df[\"High_lag1\"]) / df[\"Open\"]\n",
    "\n",
    "df[\"Open_Close_200_MA\"] = (df[\"Open\"]- df[\"Close_lag1_200_MA\"]) / df[\"Open\"]\n",
    "df[\"Open_Open_200_MA\"] = (df[\"Open\"]- df[\"Open_lag1_200_MA\"]) / df[\"Open\"]\n",
    "df[\"Open_Low_200_MA\"] = (df[\"Open\"]- df[\"Low_lag1_200_MA\"]) / df[\"Open\"]\n",
    "df[\"Open_High_200_MA\"] = (df[\"Open\"]- df[\"High_lag1_200_MA\"]) / df[\"Open\"]\n",
    "\n",
    "df[\"Open_Close_100_MA\"] = (df[\"Open\"]- df[\"Close_lag1_100_MA\"]) / df[\"Open\"]\n",
    "df[\"Open_Open_100_MA\"] = (df[\"Open\"]- df[\"Open_lag1_100_MA\"]) / df[\"Open\"]\n",
    "df[\"Open_Low_100_MA\"] = (df[\"Open\"]- df[\"Low_lag1_100_MA\"]) / df[\"Open\"]\n",
    "df[\"Open_High_100_MA\"] = (df[\"Open\"]- df[\"High_lag1_100_MA\"]) / df[\"Open\"]\n",
    "\n",
    "df[\"Open_Close_10_MA\"] = (df[\"Open\"]- df[\"Close_lag1_10_MA\"]) / df[\"Open\"]\n",
    "df[\"Open_Open_10_MA\"] = (df[\"Open\"]- df[\"Open_lag1_10_MA\"]) / df[\"Open\"]\n",
    "df[\"Open_Low_10_MA\"] = (df[\"Open\"]- df[\"Low_lag1_10_MA\"]) / df[\"Open\"]\n",
    "df[\"Open_High_10_MA\"] = (df[\"Open\"]- df[\"High_lag1_10_MA\"]) / df[\"Open\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fa7d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['EMA_Close'] = ta.trend.ema_indicator(df['Close'])\n",
    "df['EMA_Ratio_Close'] = (df['Close'] - df['EMA_Close']) / df['Close']\n",
    "bollinger = ta.volatility.BollingerBands(close=df['Close'])\n",
    "df['BB_Bandwidth'] = (bollinger.bollinger_hband() - bollinger.bollinger_lband()) / bollinger.bollinger_mavg()\n",
    "df['BB_Percent'] = (df['Close'] - bollinger.bollinger_lband()) / (bollinger.bollinger_hband() - bollinger.bollinger_lband())/2\n",
    "stochastic = ta.momentum.stoch(df['High'], df['Low'], df['Close'])\n",
    "df['Stochastic_Scaled'] = stochastic / 100.0\n",
    "df['EMA_Ratio_Close'] = df['EMA_Ratio_Close'].shift(1)\n",
    "df['BB_Bandwidth'] = df['BB_Bandwidth'].shift(1)\n",
    "df['BB_Percent'] = df['BB_Percent'].shift(1)\n",
    "df['Stochastic_Scaled']  = df['Stochastic_Scaled'].shift(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9b6704",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SMA_5'] = ta.trend.SMAIndicator(close=df['Close'], window=5).sma_indicator()\n",
    "df['SMA_10'] = ta.trend.SMAIndicator(close=df['Close'], window=10).sma_indicator()\n",
    "df['MA_Crossover_Signal'] = 0\n",
    "df.loc[df['SMA_5'] > df['SMA_10'], 'MA_Crossover_Signal'] = 1  # Bullish crossover\n",
    "df.loc[df['SMA_5'] < df['SMA_10'], 'MA_Crossover_Signal'] = -1  # Bearish crossover\n",
    "df['MA_Crossover'] = (df['SMA_5']-df['SMA_10'])/df['SMA_10']\n",
    "df['MA_Crossover'] = df['MA_Crossover'].shift(1)\n",
    "df['MA_Crossover_Signal'] = df['MA_Crossover_Signal'].shift(1)\n",
    "df = df.drop(columns=['EMA_Close','SMA_5','SMA_10',\"MA_Crossover\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df, x=\"Date\", y=[\"Stochastic_Scaled\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b46156",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_and_concat_data(api_key):\n",
    "    base_url = \"https://api.twelvedata.com/time_series\"\n",
    "    #start_date = datetime(year=2023, month=12, day=1)\n",
    "    start_date = datetime(year=2019, month=10, day=30)\n",
    "    end_date = datetime.now()  # Or any other end date you want\n",
    "\n",
    "    all_data_frames = []\n",
    "    request_count = 0\n",
    "\n",
    "    while start_date < end_date:\n",
    "        params = {\n",
    "            \"symbol\": \"GDAXI\",\n",
    "            \"interval\": \"5min\",\n",
    "            \"format\": \"CSV\",\n",
    "            \"apikey\": api_key,\n",
    "            \"start_date\": start_date.strftime(\"%m/%d/%Y 9:00\"),\n",
    "            \"end_date\": (start_date + timedelta(days=1)).strftime(\"%m/%d/%Y 17:31\")\n",
    "        }\n",
    "\n",
    "        response = requests.get(base_url, params=params)\n",
    "        request_count += 1\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            csv_data = StringIO(response.text)\n",
    "            df = pd.read_csv(csv_data, delimiter=';')\n",
    "            df = df.iloc[::-1]\n",
    "            all_data_frames.append(df)\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for {start_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "        # Check if rate limit is reached\n",
    "        if request_count >= 55:\n",
    "            time.sleep(60)  # Sleep for 60 seconds\n",
    "            request_count = 0  # Reset request count\n",
    "\n",
    "        start_date += timedelta(days=1)\n",
    "\n",
    "    # Concatenate all data frames\n",
    "    final_df = pd.concat(all_data_frames, ignore_index=True)\n",
    "\n",
    "    # Remove duplicates based on the datetime column\n",
    "    final_df.drop_duplicates(subset='datetime', keep='first', inplace=True)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "# Usage\n",
    "api_key = \"REDACTED_API_KEY\"\n",
    "data_frame = fetch_and_concat_data(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0394a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "intradaydax = data_frame[[\"datetime\",\"open\",\"high\",\"low\",\"close\"]]\n",
    "intradaydax.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f60fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "intradaydax['datetime'] = pd.to_datetime(intradaydax['datetime'])\n",
    "intradaydax.set_index('datetime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484eaf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate daily candles updated at every 5-minute interval\n",
    "def calculate_updated_daily_candles(df):\n",
    "    # Create an empty DataFrame for the updated daily candles\n",
    "    updated_daily_candles = pd.DataFrame(columns=['open', 'high', 'low', 'close'], index=df.index)\n",
    "\n",
    "    # Iterate over the 5-minute candles\n",
    "    for current_time in df.index:\n",
    "        # Filter data up to the current timestamp\n",
    "        current_day_data = df[df.index.date == current_time.date()]\n",
    "        up_to_current_time_data = current_day_data[current_day_data.index <= current_time]\n",
    "\n",
    "        # Calculate updated daily candle\n",
    "        updated_daily_candles.loc[current_time, 'open'] = current_day_data.iloc[0]['open']\n",
    "        updated_daily_candles.loc[current_time, 'high'] = up_to_current_time_data['high'].max()\n",
    "        updated_daily_candles.loc[current_time, 'low'] = up_to_current_time_data['low'].min()\n",
    "        updated_daily_candles.loc[current_time, 'close'] = up_to_current_time_data.iloc[-1]['close']\n",
    "\n",
    "    return updated_daily_candles\n",
    "\n",
    "# Calculate the updated daily candles\n",
    "intradaydax_agg = calculate_updated_daily_candles(intradaydax)\n",
    "\n",
    "# Display the first few rows of the updated daily candles dataframe\n",
    "intradaydax_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee53977",
   "metadata": {},
   "outputs": [],
   "source": [
    "intradaydax_agg = intradaydax_agg.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e168d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "intradaydax_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae09dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "intradaydax_agg['Date'] = intradaydax_agg['datetime'].dt.date\n",
    "intradaydax_agg['Date'] = pd.to_datetime(intradaydax_agg['Date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c6c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "intradaydax_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4ac86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "intradaydax_agg[intradaydax_agg[\"Date\"]== \"2023-12-22\"].iloc[0:17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674fb25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "intradaydax_agg =intradaydax_agg.rename(columns={\"high\": \"High\", \"low\": \"Low\",\"open\": \"Open\",\"close\": \"Close\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pricelong(indexvalue, knockoutprice) :\n",
    "    \"\"\"Calculate the price of long options \"\"\"\n",
    "    return np.maximum((indexvalue - knockoutprice) * 0.01, 0)\n",
    "\n",
    "def priceshort(indexvalue, knockoutprice) :\n",
    "    \"\"\"Calculate the price of short options\"\"\"\n",
    "    return np.maximum((knockoutprice - indexvalue) * 0.01, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8e681e",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def prepare_intra_day_data(date):\n",
    "    \"\"\"\n",
    "    Prepares intra-day data for a given date.\n",
    "    \"\"\"\n",
    "    # Load the data for the given date\n",
    "    open_day = df[df[\"Date\"] == date][\"Open\"]\n",
    "    knockout_long = 0.97 * open_day\n",
    "    knockout_short = 1.03 * open_day\n",
    "    open_price_long = pricelong(open_day,knockout_long) \n",
    "    open_price_short = priceshort(open_day,knockout_short)\n",
    "    df_intra_day = intradaydax_agg[intradaydax_agg[\"Date\"]==date]\n",
    "    temp_list = []\n",
    "    for element in df_intra_day.iloc[0:17].iterrows():\n",
    "        high_long = pricelong(element[1][\"High\"],knockout_long)\n",
    "        low_long = pricelong(element[1][\"Low\"],knockout_long)\n",
    "        close_long = pricelong(element[1][\"Close\"],knockout_long)\n",
    "\n",
    "        high_short = priceshort(element[1][\"High\"],knockout_short)\n",
    "        low_short = priceshort(element[1][\"Low\"],knockout_short)\n",
    "        close_short = priceshort(element[1][\"Close\"],knockout_short)\n",
    "        perc_high_long = (high_long - open_price_long) / open_price_long\n",
    "        perc_low_long = (low_long - open_price_long) / open_price_long\n",
    "        perc_close_long = (close_long - open_price_long) / open_price_long\n",
    "        perc_high_short = (high_short - open_price_short) / open_price_short\n",
    "        perc_low_short = (low_short - open_price_short) / open_price_short\n",
    "        perc_close_short = (close_short - open_price_short) / open_price_short\n",
    "\n",
    "        temp_list.append([perc_high_long.iloc[0],perc_low_long.iloc[0],perc_close_long.iloc[0],perc_high_short.iloc[0],perc_low_short.iloc[0],perc_close_short.iloc[0]])\n",
    "   \n",
    "    \n",
    "    \n",
    "    return temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec3ba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "test.append([12,23])\n",
    "test.append([12,23233])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac74892",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(test).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccba556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4470b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5c03fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(prepare_intra_day_data(\"2023-12-22\")).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad82b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_daily_data = []\n",
    "for elem in intradaydax_agg[\"Date\"].unique():\n",
    "    all_daily_data.append(prepare_intra_day_data(elem))\n",
    "all_daily_data = np.array(all_daily_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e49901",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_daily_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba292b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findtarget(date):\n",
    "    close_day = df[df[\"Date\"] == date][\"Close\"].values[0]\n",
    "    high_day = df[df[\"Date\"] == date][\"Close\"].values[0]\n",
    "    low_day = df[df[\"Date\"] == date][\"Close\"].values[0]\n",
    "    df_intra_day = intradaydax_agg[intradaydax_agg[\"Date\"]==date]\n",
    "\n",
    "    index_value = df_intra_day.iloc[19][\"Close\"]\n",
    "    \n",
    "\n",
    "    knockout_long = 0.97 * index_value\n",
    "    knockout_short = 1.03 * index_value\n",
    "\n",
    "    open_price_long = pricelong(index_value,knockout_long) \n",
    "     \n",
    "    close_price_long = pricelong(close_day,knockout_long)\n",
    "     \n",
    "\n",
    "    if close_price_long > open_price_long:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da7dcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "findtarget(\"2023-12-22\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1519338",
   "metadata": {},
   "outputs": [],
   "source": [
    "intradaydax_agg[intradaydax_agg[\"Date\"]==\"2023-12-22\"][\"Close\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6af6cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "close_day = df[df[\"Date\"] == \"2023-12-22\"][\"Close\"].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887d007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_targets = []\n",
    "for elem in intradaydax_agg[\"Date\"].unique():\n",
    "    all_targets.append(findtarget(elem))\n",
    "all_targets = np.array(all_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45fa3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43bcb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.bincount(all_targets)\n",
    "num_zeros = counts[0]\n",
    "num_ones = counts[1]\n",
    "\n",
    "# Calculate proportions\n",
    "total_elements = all_targets.size\n",
    "proportion_zeros = num_zeros / total_elements\n",
    "proportion_ones = num_ones / total_elements\n",
    "\n",
    "# Print the results\n",
    "print(\"Number of 0s:\", num_zeros)\n",
    "print(\"Number of 1s:\", num_ones)\n",
    "print(\"Proportion of 0s:\", proportion_zeros)\n",
    "print(\"Proportion of 1s:\", proportion_ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a6a27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d5df40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_min_deviation(date):\n",
    "    high_day = df[df[\"Date\"] == date][\"High\"].values[0]\n",
    "    low_day = df[df[\"Date\"] == date][\"Low\"].values[0]\n",
    "    high_day_intra_day = intradaydax_agg[intradaydax_agg[\"Date\"] == date][\"High\"].max()\n",
    "    low_day_intra_day = intradaydax_agg[intradaydax_agg[\"Date\"] == date][\"Low\"].min()\n",
    "    return [ np.abs((high_day-high_day_intra_day )/high_day),np.abs((low_day_intra_day-low_day)/low_day) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f37d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_max_min_deviation(\"2023-12-22\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b624fa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "deviations = []\n",
    "for elem in intradaydax_agg[\"Date\"].unique():\n",
    "    deviations.append(get_max_min_deviation(elem))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b786939",
   "metadata": {},
   "outputs": [],
   "source": [
    "deviations = np.array(deviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21d09c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "deviations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7e8f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max( deviations[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fafdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sqrt(np.var( deviations[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5833cc92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16044987",
   "metadata": {},
   "outputs": [],
   "source": [
    "close_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dba8b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7ea663",
   "metadata": {},
   "outputs": [],
   "source": [
    "intradaydax_agg.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98869bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df = intradaydax.resample('D').agg({'open': 'first', \n",
    "                                 'high': 'max', \n",
    "                                 'low': 'min', \n",
    "                                 'close': 'last'})\n",
    "\n",
    "# Drop rows with NaN values (days where there might be no data)\n",
    "daily_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba506bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dee99de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weekday'] = df[\"Date\"].dt.weekday.astype(np.int8) \n",
    "df['month'] = df[\"Date\"].dt.month.astype(np.int8) - 1\n",
    "df['monthday'] = df[\"Date\"].dt.day.astype(np.int8) -1\n",
    "df['month_of_quarter'] = ((df[\"Date\"].dt.month - 1) % 3) \n",
    "df = one_hot_encode_column(df,'weekday')\n",
    "df = one_hot_encode_column(df,'month') \n",
    "df = one_hot_encode_column(df,'monthday')\n",
    "df = one_hot_encode_column(df,'month_of_quarter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c9166",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be5998e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30ca263",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Open', 'High', 'Low',\"Close\",'Open_lag1', 'High_lag1', 'Low_lag1',\"Close_lag1\"]\n",
    "columns_delete = []\n",
    "\n",
    "for elem in columns:\n",
    "    columns_delete.append(f\"{elem}\")\n",
    "    columns_delete.append(f\"{elem}_200_MA\")\n",
    "    columns_delete.append(f\"{elem}_100_MA\")\n",
    "    columns_delete.append(f\"{elem}_10_MA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef65041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[columns_delete]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e07546",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fd146a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cb41d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset= df.drop(columns_delete,axis=1)\n",
    "dataset = dataset.iloc[:-1]\n",
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea03699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e3fa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset = dataset[dataset['Date'].isin(intradaydax_agg['Date'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ef2687",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_not_in_dataset = intradaydax_agg[~intradaydax_agg['Date'].isin(dataset['Date'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc55e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_not_in_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53fcce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = filtered_dataset.values[:,1:]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc3c7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cded2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3db5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_daily_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07936a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = data\n",
    "X2 = all_daily_data[:-1,:]\n",
    "y = all_targets[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b14bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0415103",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78426b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc531230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8135f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each array individually while keeping the same random_state to ensure matching indices\n",
    "X1_train, X1_test, y_train, y_test = train_test_split(X1, y, test_size=0.2, random_state=42)\n",
    "X2_train, X2_test, _ , _ = train_test_split(X2, y, test_size=0.2, random_state=42)  # y is just to keep the split consistent\n",
    "data_train, data_test, _ , _ = train_test_split(filtered_dataset.values, y, test_size=0.2, random_state=42)  # y is just to keep the split consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8486c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dedb954",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ded122f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout,LeakyReLU,BatchNormalization, concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from ncps import wirings\n",
    "from ncps.tf import LTC\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3ce838",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_wiring = wirings.FullyConnected(64, 2)\n",
    "fc_wiring2 = wirings.FullyConnected(64, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70040f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "input_model_X1 = Input(shape= X1_train.shape[1:3])\n",
    "denseone = Dense(64,)(input_model_X1)\n",
    "denseone = LeakyReLU(alpha=0.3)(denseone)\n",
    "denseone = Dropout(rate=0.3)(denseone)\n",
    "denseone= BatchNormalization()(denseone)\n",
    "    \n",
    "densetwo = Dense(64,)(denseone)\n",
    "densetwo = LeakyReLU(alpha=0.3)(densetwo)\n",
    "densetwo = Dropout(rate=0.3)(densetwo)\n",
    "densetwo= BatchNormalization()(densetwo)\n",
    "final = Dense(1, name='Dense_final', activation='sigmoid')(densetwo)\n",
    "\n",
    "input_model_X2 = Input(shape= X2_train.shape[1:3])\n",
    "LTC(fc_wiring, return_sequences=True)(input_model_X2)\n",
    "#lstm1 = LSTM(64, return_sequences=True)(input_model_X2)\n",
    "#lstm1 = Dropout(rate=0.3)(lstm1)\n",
    "#lstm1 = LeakyReLU(alpha=0.3)(lstm1)\n",
    "\n",
    "lstm2 = LSTM(64)(lstm1)\n",
    "lstm2 = Dropout(rate=0.3)(lstm2)\n",
    "lstm2 = LeakyReLU(alpha=0.3)(lstm2)\n",
    "final2 = Dense(1, name='Dense_final2', activation='sigmoid')(lstm2)\n",
    "\n",
    "mergelayer = concatenate([final, final2])\n",
    "\n",
    "densemerge = Dense(64)(mergelayer)\n",
    "densemerge = LeakyReLU(alpha=0.3)(densemerge)\n",
    "densemerge = Dropout(rate=0.3)(densemerge)\n",
    "densemerge= BatchNormalization()(densemerge)\n",
    "finalmerge = Dense(1, name='Dense_finalmerge', activation='sigmoid')(densemerge)\n",
    "\n",
    "model = Model(inputs=[input_model_X1, input_model_X2], outputs=finalmerge)\n",
    "model.compile(loss=keras.losses.BinaryCrossentropy(), optimizer=Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de86820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95994702",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train = X1_train.astype(np.float32)\n",
    "X2_train = X2_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "X1_test = X1_test.astype(np.float32)\n",
    "X2_test = X2_test.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29285b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history =model.fit([X1_train,X2_train],y_train,epochs=500,validation_data=([X1_test,X2_test], y_test),batch_size=32,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc184b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict =model.predict([X1_test,X2_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910b9519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve\n",
    "import plotly.graph_objects as go\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8335113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketmodels = \"REDACTED_BUCKET\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8e3475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file_to_s3(bucket, key, file):\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    s3.upload_file(file, bucket, key)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d5a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_to_s3(bucket, key, content):\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    s3.put_object(Bucket=bucket, Key=key, Body=content)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7f5081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_strategy(date, direction):\n",
    "    close_day = df[df[\"Date\"] == date][\"Close\"].values[0]\n",
    "    high_day = df[df[\"Date\"] == date][\"Close\"].values[0]\n",
    "    low_day = df[df[\"Date\"] == date][\"Close\"].values[0]\n",
    "    df_intra_day = intradaydax_agg[intradaydax_agg[\"Date\"]==date]\n",
    "\n",
    "    index_value = df_intra_day.iloc[19][\"Close\"]\n",
    "    \n",
    "\n",
    "    knockout_long = 0.97 * index_value\n",
    "    knockout_short = 1.03 * index_value\n",
    "\n",
    "    open_price_long = pricelong(index_value,knockout_long)\n",
    "    open_price_short = priceshort(index_value,knockout_short) \n",
    "     \n",
    "    close_price_long = pricelong(close_day,knockout_long)\n",
    "    close_price_short = priceshort(close_day,knockout_short)\n",
    "\n",
    "    if close_price_long < 0.71 *open_price_long:\n",
    "        close_price_long = 0.71 *open_price_long\n",
    "    if close_price_short < 0.71 *open_price_short:\n",
    "        close_price_short = 0.71 *open_price_short \n",
    "    \n",
    "    if direction == 1:\n",
    "        return 1+ (close_price_long - open_price_long) / open_price_long\n",
    "    else:\n",
    "        return 1+(close_price_short - open_price_short) / open_price_short\n",
    "     \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d031b4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_ITERATIONS = 10000\n",
    "LOG_INTERVAL = 100\n",
    "start_time = time.time()\n",
    "def optimizeml(**kwargs):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "    global performance, prediction, counter\n",
    "    p1 = int(kwargs[\"x1\"])\n",
    "    drop1 = kwargs[\"drop1\"]\n",
    "    l1 = kwargs[\"leaky1\"]\n",
    "   \n",
    "    p2 = int(kwargs[\"x2\"])\n",
    "    l2 = kwargs[\"leaky2\"]\n",
    "    drop2 = kwargs[\"drop2\"]\n",
    "    \n",
    "    p3 = int(kwargs[\"x3\"])\n",
    "    l3 = kwargs[\"leaky3\"]\n",
    "    drop3 = kwargs[\"drop3\"]\n",
    "    \n",
    "    lstm1p1 = int(kwargs[\"lstm1\"])\n",
    "    lstml1 = kwargs[\"lstmleaky1\"]\n",
    "    lstm1drop1 = kwargs[\"lstmdrop1\"]\n",
    "    \n",
    "    lstm2p2 = int(kwargs[\"lstm2\"])\n",
    "    lstml2 = kwargs[\"lstmleaky2\"]\n",
    "    lstm2drop2 = kwargs[\"lstmdrop2\"]\n",
    "    \n",
    "    epoch =int(kwargs[\"epoch\"])\n",
    "    batch_size =int(kwargs[\"batch_size\"])\n",
    "    \n",
    "    input_model_X1 = Input(shape= X1_train.shape[1:3])\n",
    "    denseone = Dense(p1,)(input_model_X1)\n",
    "    denseone = LeakyReLU(alpha=l1)(denseone)\n",
    "    denseone = Dropout(rate=drop1)(denseone)\n",
    "    denseone= BatchNormalization()(denseone)\n",
    "    \n",
    "    densetwo = Dense(p2,)(denseone)\n",
    "    densetwo = LeakyReLU(alpha=l2)(densetwo)\n",
    "    densetwo = Dropout(rate=drop2)(densetwo)\n",
    "    densetwo= BatchNormalization()(densetwo)\n",
    "    final = Dense(1, name='Dense_final', activation='sigmoid')(densetwo)\n",
    "\n",
    "    input_model_X2 = Input(shape= X2_train.shape[1:3])\n",
    "    lstm1 = LSTM(lstm1p1, return_sequences=True)(input_model_X2)\n",
    "    lstm1 = Dropout(rate=lstml1)(lstm1)\n",
    "    lstm1 = LeakyReLU(alpha=lstm1drop1)(lstm1)\n",
    "\n",
    "    lstm2 = LSTM(lstm2p2)(lstm1)\n",
    "    lstm2 = Dropout(rate=lstm2drop2)(lstm2)\n",
    "    lstm2 = LeakyReLU(alpha=lstml2)(lstm2)\n",
    "    final2 = Dense(1, name='Dense_final2', activation='sigmoid')(lstm2)\n",
    "\n",
    "    mergelayer = concatenate([final, final2])\n",
    "\n",
    "    densemerge = Dense(p3)(mergelayer)\n",
    "    densemerge = LeakyReLU(alpha=l3)(densemerge)\n",
    "    densemerge = Dropout(rate=drop3)(densemerge)\n",
    "    densemerge= BatchNormalization()(densemerge)\n",
    "    finalmerge = Dense(1, name='Dense_finalmerge', activation='sigmoid')(densemerge)\n",
    "\n",
    "    model = Model(inputs=[input_model_X1, input_model_X2], outputs=finalmerge)\n",
    "    model.compile(loss=keras.losses.BinaryCrossentropy(), optimizer=Adam())\n",
    "    \n",
    "    \n",
    "    history =model.fit([X1_train,X2_train],y_train,epochs=epoch,validation_data=([X1_test,X2_test], y_test),batch_size=batch_size,verbose=0)\n",
    "    loss = history.history[\"val_loss\"]\n",
    "    last_val_loss = loss[len(loss)-1]\n",
    "    y_test_predict =model.predict([X1_test,X2_test],verbose=0)\n",
    "    y_pred_class = (y_test_predict > 0.5).astype(\"int32\")  # Convert probabilities to class labels\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_test_predict)\n",
    "    roc_auc = float(auc(fpr, tpr))\n",
    "    \n",
    "    strat_performance = 1\n",
    "    for i, elem in enumerate(data_test[:,0]):\n",
    "        direction = y_pred_class[i,0]\n",
    "        temp_performance = evaluate_strategy(elem, direction)\n",
    "        strat_performance *= temp_performance\n",
    "    \n",
    "\n",
    "\n",
    "    if strat_performance > performance:\n",
    "        logger.info(f\"New performance is {strat_performance} and it is better than the previous one {performance} \")  \n",
    "        performance = strat_performance\n",
    "        model.save_weights(f\"interdaymodel.h5f\", overwrite=True)\n",
    "        upload_file_to_s3(bucketmodels,f\"confidencemodelday/interdaymodel.h5f.data-00000-of-00001\", f\"interdaymodel.h5f.data-00000-of-00001\")\n",
    "        upload_file_to_s3(bucketmodels,f\"confidencemodelday/interdaymodel.h5f.index\",f\"interdaymodel.h5f.index\")\n",
    "        write_data_to_s3(bucketmodels, f\"confidencemodelday/interdaymodel.csv\", json.dumps(kwargs ))\n",
    "        \n",
    "    counter += 1\n",
    "    if counter % LOG_INTERVAL == 0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        avg_speed = counter / elapsed_time * 60  # iterations per minute\n",
    "        \n",
    "        remaining_iterations = TOTAL_ITERATIONS - counter\n",
    "        estimated_completion_time = remaining_iterations / avg_speed  /60\n",
    "        \n",
    "        logger.info(f\"Completed {counter/TOTAL_ITERATIONS*100}% of the iterations.\")\n",
    "        logger.info(f\"Average speed: {avg_speed:.2f} iterations/minute.\")\n",
    "        logger.info(f\"Estimated time to completion: {estimated_completion_time:.2f} hours.\")\n",
    "    return strat_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a21023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2250546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cfc810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "optimizer = BayesianOptimization(\n",
    "    f=optimizeml,\n",
    "     pbounds={'x1': (1, 128),'x2': (1, 128),'x3': (1, 128),'x4': (1, 128),\n",
    " 'drop1': (0, 0.5),'drop2': (0, 0.5),'drop3': (0, 0.5),'drop4': (0, 0.5),\n",
    " 'leaky1': (0, 1),'leaky2': (0, 1),'leaky3': (0, 1),'leaky4': (0, 1),\n",
    " \"lstm1\": (1, 128),'lstm2': (1, 128),\n",
    " \"lstmleaky1\" : (0, 1),'lstmleaky2': (0, 1),\n",
    " \"lstmdrop1\": (0, 0.5),'lstmdrop2': (0, 0.5),\n",
    " 'epoch': (10, 200),\n",
    " 'batch_size': (2, 256)},\n",
    "    verbose=0,\n",
    "    random_state=42,\n",
    "    #bounds_transformer=bounds_transformer\n",
    ")\n",
    "performance = 0\n",
    "counter = 0\n",
    "optimizer.maximize(\n",
    "    init_points=5,\n",
    "    n_iter=TOTAL_ITERATIONS-5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae62a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(kwargs):\n",
    "    \n",
    "    tf.keras.backend.clear_session()\n",
    "     \n",
    "    p1 = int(kwargs[\"x1\"])\n",
    "    drop1 = kwargs[\"drop1\"]\n",
    "    l1 = kwargs[\"leaky1\"]\n",
    "   \n",
    "    p2 = int(kwargs[\"x2\"])\n",
    "    l2 = kwargs[\"leaky2\"]\n",
    "    drop2 = kwargs[\"drop2\"]\n",
    "    \n",
    "    p3 = int(kwargs[\"x3\"])\n",
    "    l3 = kwargs[\"leaky3\"]\n",
    "    drop3 = kwargs[\"drop3\"]\n",
    "    \n",
    "    lstm1p1 = int(kwargs[\"lstm1\"])\n",
    "    lstml1 = kwargs[\"lstmleaky1\"]\n",
    "    lstm1drop1 = kwargs[\"lstmdrop1\"]\n",
    "    \n",
    "    lstm2p2 = int(kwargs[\"lstm2\"])\n",
    "    lstml2 = kwargs[\"lstmleaky2\"]\n",
    "    lstm2drop2 = kwargs[\"lstmdrop2\"]\n",
    "    \n",
    "    epoch =int(kwargs[\"epoch\"])\n",
    "    batch_size =int(kwargs[\"batch_size\"])\n",
    "    \n",
    "    input_model_X1 = Input(shape= X1_train.shape[1:3])\n",
    "    denseone = Dense(p1,)(input_model_X1)\n",
    "    denseone = LeakyReLU(alpha=l1)(denseone)\n",
    "    denseone = Dropout(rate=drop1)(denseone)\n",
    "    denseone= BatchNormalization()(denseone)\n",
    "    \n",
    "    densetwo = Dense(p2,)(denseone)\n",
    "    densetwo = LeakyReLU(alpha=l2)(densetwo)\n",
    "    densetwo = Dropout(rate=drop2)(densetwo)\n",
    "    densetwo= BatchNormalization()(densetwo)\n",
    "    final = Dense(1, name='Dense_final', activation='sigmoid')(densetwo)\n",
    "\n",
    "    input_model_X2 = Input(shape= X2_train.shape[1:3])\n",
    "    lstm1 = LSTM(lstm1p1, return_sequences=True)(input_model_X2)\n",
    "    lstm1 = Dropout(rate=lstml1)(lstm1)\n",
    "    lstm1 = LeakyReLU(alpha=lstm1drop1)(lstm1)\n",
    "\n",
    "    lstm2 = LSTM(lstm2p2)(lstm1)\n",
    "    lstm2 = Dropout(rate=lstm2drop2)(lstm2)\n",
    "    lstm2 = LeakyReLU(alpha=lstml2)(lstm2)\n",
    "    final2 = Dense(1, name='Dense_final2', activation='sigmoid')(lstm2)\n",
    "\n",
    "    mergelayer = concatenate([final, final2])\n",
    "\n",
    "    densemerge = Dense(p3)(mergelayer)\n",
    "    densemerge = LeakyReLU(alpha=l3)(densemerge)\n",
    "    densemerge = Dropout(rate=drop3)(densemerge)\n",
    "    densemerge= BatchNormalization()(densemerge)\n",
    "    finalmerge = Dense(1, name='Dense_finalmerge', activation='sigmoid')(densemerge)\n",
    "\n",
    "    model = Model(inputs=[input_model_X1, input_model_X2], outputs=finalmerge)\n",
    "    model.compile(loss=keras.losses.BinaryCrossentropy(), optimizer=Adam())\n",
    "    model.load_weights(\"interdaymodel.h5f\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92a2482",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b7d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource('s3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801e1776",
   "metadata": {},
   "outputs": [],
   "source": [
    "dax = (s3.Object(\"REDACTED_BUCKET\", \"confidencemodelday/interdaymodel.csv\").get()['Body'].read().decode('utf-8') )\n",
    "analysisfile = open(\"interdaymodel.csv\", \"w+\")\n",
    "analysisfile.write(dax)\n",
    "analysisfile.close()\n",
    "dax = (s3.Object(\"REDACTED_BUCKET\", \"confidencemodelday/interdaymodel.h5f.index\").get()['Body'].read() )\n",
    "analysisfile = open(\"interdaymodel.h5f.index\", 'wb')\n",
    "analysisfile.write(dax)\n",
    "analysisfile.close()\n",
    "dax = (s3.Object(\"REDACTED_BUCKET\", \"confidencemodelday/interdaymodel.h5f.data-00000-of-00001\").get()['Body'].read() )\n",
    "analysisfile = open(\"interdaymodel.h5f.data-00000-of-00001\", 'wb')\n",
    "analysisfile.write(dax)\n",
    "analysisfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34342322",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = json.loads(open(\"interdaymodel.csv\", 'r').read())\n",
    "schema\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66ccc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(schema) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316a9661",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a48d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict =model.predict([X1_test,X2_test],verbose=0)\n",
    "y_pred_class = (y_test_predict > 0.5).astype(\"int32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e75af40",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predict[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e79f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_class)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines', name='ROC curve (area = %0.2f)' % roc_auc))\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='No Skill'))\n",
    "fig.update_layout(xaxis_title='False Positive Rate', yaxis_title='True Positive Rate', title='Receiver Operating Characteristic')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21085a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_strategy(\"2023-12-12\", 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad8abd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = 1\n",
    "for i, elem in enumerate(data_test[:,0]):\n",
    "    direction = y_pred_class[i,0]\n",
    "    temp_performance = evaluate_strategy(elem, direction)\n",
    "    performance *= temp_performance\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa910a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79008ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8295783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train.shape[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642323ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = roc_auc_score(y_test, y_test_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e470d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ROC AUC: {roc_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103146b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_predict)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines', name='ROC curve (area = %0.2f)' % roc_auc))\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='No Skill'))\n",
    "fig.update_layout(xaxis_title='False Positive Rate', yaxis_title='True Positive Rate', title='Receiver Operating Characteristic')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d9afc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eef3f533",
   "metadata": {},
   "source": [
    "# Let's try a more subtle approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea95d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d867411",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1[len(X1)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0e380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2[len(X1)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5676548",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "largelanguagemodel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
