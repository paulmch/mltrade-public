{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c72f995",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder \n",
    "import boto3\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split\n",
    "import requests\n",
    "from io import StringIO\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import sys\n",
    "import logging\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=SettingWithCopyWarning)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb806c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', handlers=[logging.StreamHandler(sys.stdout)])\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b46156",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "def fetch_and_concat_data(api_key):\n",
    "    base_url = \"https://financialmodelingprep.com/api/v3/historical-chart/5min/^NDX\"\n",
    "    start_date = datetime(year=2019, month=10, day=30)\n",
    "    end_date = datetime.now()  # Or any other end date you want\n",
    "\n",
    "    all_data_frames = []\n",
    "    request_count = 0\n",
    "    requests_per_minute = 200\n",
    "    sleep_time = 60 / requests_per_minute  # Adjust sleep time to maintain rate limit\n",
    "    total_days = (end_date - start_date).days\n",
    "\n",
    "    with tqdm(total=total_days, desc=\"Fetching data\") as pbar:\n",
    "        while start_date < end_date:\n",
    "            params = {\n",
    "                \"apikey\": api_key,\n",
    "                \"from\": start_date.strftime(\"%Y-%m-%d\"),\n",
    "                \"to\": start_date.strftime(\"%Y-%m-%d\")\n",
    "            }\n",
    "\n",
    "            response = requests.get(base_url, params=params)\n",
    "            request_count += 1\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if data:  # Check if data is not empty\n",
    "                    df = pd.DataFrame(data)\n",
    "                    df['date'] = pd.to_datetime(df['date'])  # Ensure the 'date' column is in datetime format\n",
    "                    df = df.iloc[::-1]  # Reverse the dataframe to maintain chronological order\n",
    "                    all_data_frames.append(df)\n",
    "            else:\n",
    "                print(f\"Failed to fetch data for {start_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "            # Sleep to maintain the rate limit\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "            start_date += timedelta(days=1)\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Concatenate all data frames\n",
    "    final_df = pd.concat(all_data_frames, ignore_index=True)\n",
    "\n",
    "    # Remove duplicates based on the datetime column\n",
    "    final_df.drop_duplicates(subset='date', keep='first', inplace=True)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "\n",
    "\n",
    "# Usage\n",
    "api_key = \"REDACTED_API_KEY\"\n",
    "data_frame = fetch_and_concat_data(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f70b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame[[\"date\",\"open\",\"high\",\"low\",\"close\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a38a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "intradayndx = data_frame[[\"date\",\"open\",\"high\",\"low\",\"close\"]]\n",
    "intradayndx = intradayndx.rename(columns={\"date\":\"datetime\"})\n",
    "intradayndx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a024d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "intradayndx['datetime'] = pd.to_datetime(intradayndx['datetime'])\n",
    "intradayndx.set_index('datetime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671a7629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate daily candles updated at every 5-minute interval\n",
    "def calculate_updated_daily_candles(df):\n",
    "    # Create an empty DataFrame for the updated daily candles\n",
    "    updated_daily_candles = pd.DataFrame(columns=['open', 'high', 'low', 'close'], index=df.index)\n",
    "\n",
    "    # Iterate over the 5-minute candles\n",
    "    for current_time in df.index:\n",
    "        # Filter data up to the current timestamp\n",
    "        current_day_data = df[df.index.date == current_time.date()]\n",
    "        up_to_current_time_data = current_day_data[current_day_data.index <= current_time]\n",
    "\n",
    "        # Calculate updated daily candle\n",
    "        updated_daily_candles.loc[current_time, 'open'] = current_day_data.iloc[0]['open']\n",
    "        updated_daily_candles.loc[current_time, 'high'] = up_to_current_time_data['high'].max()\n",
    "        updated_daily_candles.loc[current_time, 'low'] = up_to_current_time_data['low'].min()\n",
    "        updated_daily_candles.loc[current_time, 'close'] = up_to_current_time_data.iloc[-1]['close']\n",
    "\n",
    "    return updated_daily_candles\n",
    "\n",
    "# Calculate the updated daily candles\n",
    "intradayndx_agg = calculate_updated_daily_candles(intradayndx)\n",
    "\n",
    "# Display the first few rows of the updated daily candles dataframe\n",
    "intradayndx_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8deb307",
   "metadata": {},
   "outputs": [],
   "source": [
    "intradayndx_agg = intradayndx_agg.reset_index()\n",
    "\n",
    "intradayndx_agg['Date'] = intradayndx_agg['datetime'].dt.date\n",
    "intradayndx_agg['Date'] = pd.to_datetime(intradayndx_agg['Date'], errors='coerce')\n",
    "intradayndx_agg =intradayndx_agg.rename(columns={\"high\": \"High\", \"low\": \"Low\",\"open\": \"Open\",\"close\": \"Close\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7343aaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df = intradayndx.resample('D').agg({'open': 'first', \n",
    "                                 'high': 'max', \n",
    "                                 'low': 'min', \n",
    "                                 'close': 'last'})\n",
    "\n",
    "# Drop rows with NaN values (days where there might be no data)\n",
    "daily_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c35e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "intradayndx_agg[intradayndx_agg[\"Date\"] == \"2024-01-18\"].iloc[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b166e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7b911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df = daily_df.reset_index()\n",
    "daily_df = daily_df.rename(columns={\"high\": \"High\", \"low\": \"Low\",\"open\": \"Open\",\"close\": \"Close\",\"datetime\": \"Date\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d24b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b945f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_buffer = StringIO()\n",
    "daily_df = daily_df.iloc[:]\n",
    "daily_df.to_csv(csv_buffer, index=False)\n",
    "# Create a boto3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Specify the bucket name and file name in S3\n",
    "bucket_name = 'REDACTED_BUCKET'\n",
    "object_name = 'ndx.csv'\n",
    "\n",
    "# Upload the CSV to S3\n",
    "#s3_client.put_object(Bucket=bucket_name, Key=object_name, Body=csv_buffer.getvalue())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6db3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = daily_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a738a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0179ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabe2cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7390b3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_column(data: pd.DataFrame, col, max_classes=None):\n",
    "    df = data.copy()\n",
    "    \n",
    "    if max_classes is not None:\n",
    "        dummies = pd.get_dummies(df[col], prefix=col, dtype=int)\n",
    "         \n",
    "        missing_cols = set(range(max_classes + 1)) - set(df[col].unique())\n",
    "        for col_num in missing_cols:\n",
    "            dummies[col +\"_\"+ str(col_num)] = 0\n",
    "        \n",
    "        # Custom sort function to ensure correct column order\n",
    "        dummies = dummies[sorted(dummies.columns, key=lambda x: int(x.split(\"_\")[1]))]\n",
    "    else:\n",
    "        dummies = pd.get_dummies(df[col], prefix=col, dtype=int)\n",
    "\n",
    "    df = pd.concat([df, dummies], axis=1)\n",
    "    if max_classes is None:\n",
    "        df = df.drop(columns=[col])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393d10d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Open_200_MA\"] = df[\"Open\"].rolling(window=30).mean()\n",
    "df[\"High_200_MA\"] = df[\"High\"].rolling(window=30).mean()\n",
    "df[\"Low_200_MA\"] = df[\"Low\"].rolling(window=30).mean()\n",
    "df[\"Close_200_MA\"] = df[\"Close\"].rolling(window=30).mean()\n",
    "df[\"Open_100_MA\"] = df[\"Open\"].rolling(window=14).mean()\n",
    "df[\"High_100_MA\"] = df[\"High\"].rolling(window=14).mean()\n",
    "df[\"Low_100_MA\"] = df[\"Low\"].rolling(window=14).mean()\n",
    "df[\"Close_100_MA\"] = df[\"Close\"].rolling(window=14).mean()\n",
    "df[\"Open_10_MA\"] = df[\"Open\"].rolling(window=7).mean()\n",
    "df[\"High_10_MA\"] = df[\"High\"].rolling(window=7).mean()\n",
    "df[\"Low_10_MA\"] = df[\"Low\"].rolling(window=7).mean()\n",
    "df[\"Close_10_MA\"] = df[\"Close\"].rolling(window=7).mean()\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2e7c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5d4d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ab74ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df, x=\"Date\", y=[\"Open\",\"Open_200_MA\",\"Open_100_MA\",\"Open_10_MA\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3724baf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ta\n",
    "\n",
    "\n",
    "# Calculate RSI\n",
    "df['RSI_open'] = ta.momentum.rsi(df['Open'])\n",
    "df['RSI_open'] = (df['RSI_open'] - 50) / 50\n",
    "df['RSI_diff_open'] = df['RSI_open'].diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d79967",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35642098",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df, x=\"Date\", y=['RSI_open'])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030f3810",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05e4cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Close_lag1'] = df['Close'].shift(1)\n",
    "df['Open_lag1'] = df['Open'].shift(1)\n",
    "df['Low_lag1'] = df['Low'].shift(1)\n",
    "df['High_lag1'] = df['High'].shift(1)\n",
    "\n",
    "df['Close_lag1_200_MA'] = df['Close_200_MA'].shift(1)\n",
    "df['Open_lag1_200_MA'] = df['Open_200_MA'].shift(1)\n",
    "df['Low_lag1_200_MA'] = df['Low_200_MA'].shift(1)\n",
    "df['High_lag1_200_MA'] = df['High_200_MA'].shift(1)\n",
    "\n",
    "df['Close_lag1_100_MA'] = df['Close_100_MA'].shift(1)\n",
    "df['Open_lag1_100_MA'] = df['Open_100_MA'].shift(1)\n",
    "df['Low_lag1_100_MA'] = df['Low_100_MA'].shift(1)\n",
    "df['High_lag1_100_MA'] = df['High_100_MA'].shift(1)\n",
    "\n",
    "df['Close_lag1_10_MA'] = df['Close_10_MA'].shift(1)\n",
    "df['Open_lag1_10_MA'] = df['Open_10_MA'].shift(1)\n",
    "df['Low_lag1_10_MA'] = df['Low_10_MA'].shift(1)\n",
    "df['High_lag1_10_MA'] = df['High_10_MA'].shift(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b60c963",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Open_Close\"] = (df[\"Open\"]- df[\"Close_lag1\"]) / df[\"Open\"]\n",
    "df[\"Open_Open\"] = (df[\"Open\"]- df[\"Open_lag1\"]) / df[\"Open\"]\n",
    "df[\"Open_Low\"] = (df[\"Open\"]- df[\"Low_lag1\"]) / df[\"Open\"]\n",
    "df[\"Open_High\"] = (df[\"Open\"]- df[\"High_lag1\"]) / df[\"Open\"]\n",
    "\n",
    "df[\"Open_Close_200_MA\"] = (df[\"Open\"]- df[\"Close_lag1_200_MA\"]) / df[\"Open\"]\n",
    "df[\"Open_Open_200_MA\"] = (df[\"Open\"]- df[\"Open_lag1_200_MA\"]) / df[\"Open\"]\n",
    "df[\"Open_Low_200_MA\"] = (df[\"Open\"]- df[\"Low_lag1_200_MA\"]) / df[\"Open\"]\n",
    "df[\"Open_High_200_MA\"] = (df[\"Open\"]- df[\"High_lag1_200_MA\"]) / df[\"Open\"]\n",
    "\n",
    "df[\"Open_Close_100_MA\"] = (df[\"Open\"]- df[\"Close_lag1_100_MA\"]) / df[\"Open\"]\n",
    "df[\"Open_Open_100_MA\"] = (df[\"Open\"]- df[\"Open_lag1_100_MA\"]) / df[\"Open\"]\n",
    "df[\"Open_Low_100_MA\"] = (df[\"Open\"]- df[\"Low_lag1_100_MA\"]) / df[\"Open\"]\n",
    "df[\"Open_High_100_MA\"] = (df[\"Open\"]- df[\"High_lag1_100_MA\"]) / df[\"Open\"]\n",
    "\n",
    "df[\"Open_Close_10_MA\"] = (df[\"Open\"]- df[\"Close_lag1_10_MA\"]) / df[\"Open\"]\n",
    "df[\"Open_Open_10_MA\"] = (df[\"Open\"]- df[\"Open_lag1_10_MA\"]) / df[\"Open\"]\n",
    "df[\"Open_Low_10_MA\"] = (df[\"Open\"]- df[\"Low_lag1_10_MA\"]) / df[\"Open\"]\n",
    "df[\"Open_High_10_MA\"] = (df[\"Open\"]- df[\"High_lag1_10_MA\"]) / df[\"Open\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fa7d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['EMA_Close'] = ta.trend.ema_indicator(df['Close'])\n",
    "df['EMA_Ratio_Close'] = (df['Close'] - df['EMA_Close']) / df['Close']\n",
    "bollinger = ta.volatility.BollingerBands(close=df['Close'])\n",
    "df['BB_Bandwidth'] = (bollinger.bollinger_hband() - bollinger.bollinger_lband()) / bollinger.bollinger_mavg()\n",
    "df['BB_Percent'] = (df['Close'] - bollinger.bollinger_lband()) / (bollinger.bollinger_hband() - bollinger.bollinger_lband())/2\n",
    "stochastic = ta.momentum.stoch(df['High'], df['Low'], df['Close'])\n",
    "df['Stochastic_Scaled'] = stochastic / 100.0\n",
    "df['EMA_Ratio_Close'] = df['EMA_Ratio_Close'].shift(1)\n",
    "df['BB_Bandwidth'] = df['BB_Bandwidth'].shift(1)\n",
    "df['BB_Percent'] = df['BB_Percent'].shift(1)\n",
    "df['Stochastic_Scaled']  = df['Stochastic_Scaled'].shift(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9b6704",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SMA_5'] = ta.trend.SMAIndicator(close=df['Close'], window=5).sma_indicator()\n",
    "df['SMA_10'] = ta.trend.SMAIndicator(close=df['Close'], window=10).sma_indicator()\n",
    "df['MA_Crossover_Signal'] = 0\n",
    "df.loc[df['SMA_5'] > df['SMA_10'], 'MA_Crossover_Signal'] = 1  # Bullish crossover\n",
    "df.loc[df['SMA_5'] < df['SMA_10'], 'MA_Crossover_Signal'] = -1  # Bearish crossover\n",
    "df['MA_Crossover'] = (df['SMA_5']-df['SMA_10'])/df['SMA_10']\n",
    "df['MA_Crossover'] = df['MA_Crossover'].shift(1)\n",
    "df['MA_Crossover_Signal'] = df['MA_Crossover_Signal'].shift(1)\n",
    "df = df.drop(columns=['EMA_Close','SMA_5','SMA_10',\"MA_Crossover\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df, x=\"Date\", y=[\"Stochastic_Scaled\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae09dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "intradayndx_agg['Date'] = intradayndx_agg['datetime'].dt.date\n",
    "intradayndx_agg['Date'] = pd.to_datetime(intradayndx_agg['Date'], errors='coerce')\n",
    "df[\"Date\"] = pd.to_datetime(df['Date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dee99de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weekday'] = df[\"Date\"].dt.weekday.astype(np.int8) \n",
    "df['month'] = df[\"Date\"].dt.month.astype(np.int8) - 1\n",
    "df['monthday'] = df[\"Date\"].dt.day.astype(np.int8) -1\n",
    "df['month_of_quarter'] = ((df[\"Date\"].dt.month - 1) % 3) \n",
    "df = one_hot_encode_column(df,'weekday')\n",
    "df = one_hot_encode_column(df,'month') \n",
    "df = one_hot_encode_column(df,'monthday')\n",
    "df = one_hot_encode_column(df,'month_of_quarter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30ca263",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Open', 'High', 'Low',\"Close\",'Open_lag1', 'High_lag1', 'Low_lag1',\"Close_lag1\"]\n",
    "columns_delete = []\n",
    "\n",
    "for elem in columns:\n",
    "    columns_delete.append(f\"{elem}\")\n",
    "    columns_delete.append(f\"{elem}_200_MA\")\n",
    "    columns_delete.append(f\"{elem}_100_MA\")\n",
    "    columns_delete.append(f\"{elem}_10_MA\")\n",
    "dataset= df.drop(columns_delete,axis=1)\n",
    "dataset = dataset.iloc[:]\n",
    "dataset = dataset.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9dc1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c6c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "intradayndx_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4ac86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "intradayndx_agg[intradayndx_agg[\"Date\"]== \"2023-12-22\"].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d0e1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pricelong(indexvalue, knockoutprice) :\n",
    "    \"\"\"Calculate the price of long options \"\"\"\n",
    "    return np.maximum((indexvalue - knockoutprice) * 0.01, 0)\n",
    "\n",
    "def priceshort(indexvalue, knockoutprice) :\n",
    "    \"\"\"Calculate the price of short options\"\"\"\n",
    "    return np.maximum((knockoutprice - indexvalue) * 0.01, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8e681e",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def prepare_intra_day_data(date):\n",
    "    \"\"\"\n",
    "    Prepares intra-day data for a given date.\n",
    "    \"\"\"\n",
    "    # Load the data for the given date\n",
    "    open_day = df[df[\"Date\"] == date][\"Open\"]\n",
    "     \n",
    "    knockout_long = 0.96 * open_day\n",
    "    knockout_short = 1.04 * open_day\n",
    "    open_price_long = pricelong(open_day,knockout_long) \n",
    "    open_price_short = priceshort(open_day,knockout_short)\n",
    "    df_intra_day = intradayndx_agg[intradayndx_agg[\"Date\"]==date]\n",
    "    temp_list = []\n",
    "    for element in df_intra_day.iloc[0:17].iterrows():\n",
    "        high_long = pricelong(element[1][\"High\"],knockout_long)\n",
    "        low_long = pricelong(element[1][\"Low\"],knockout_long)\n",
    "        close_long = pricelong(element[1][\"Close\"],knockout_long)\n",
    "\n",
    "        high_short = priceshort(element[1][\"High\"],knockout_short)\n",
    "        low_short = priceshort(element[1][\"Low\"],knockout_short)\n",
    "        close_short = priceshort(element[1][\"Close\"],knockout_short)\n",
    "        perc_high_long = (high_long - open_price_long) / open_price_long\n",
    "        perc_low_long = (low_long - open_price_long) / open_price_long\n",
    "        perc_close_long = (close_long - open_price_long) / open_price_long\n",
    "        perc_high_short = (high_short - open_price_short) / open_price_short\n",
    "        perc_low_short = (low_short - open_price_short) / open_price_short\n",
    "        perc_close_short = (close_short - open_price_short) / open_price_short\n",
    "\n",
    "        temp_list.append([perc_high_long.iloc[0],perc_low_long.iloc[0],perc_close_long.iloc[0],perc_high_short.iloc[0],perc_low_short.iloc[0],perc_close_short.iloc[0]])\n",
    "   \n",
    "    \n",
    "    \n",
    "    return temp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bd2203",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset[\"Date\"] < \"2024-01-1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad82b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = datetime.now()\n",
    "\n",
    "# Calculate the date six months ago\n",
    "six_months_ago = current_date - timedelta(days=5*30)\n",
    "\n",
    "six_months_ago = eval(\"datetime(2024, 6, 11, 10, 21, 17, 817147)\")- timedelta(days=10)\n",
    "all_daily_data = []\n",
    "\n",
    "for elem in dataset[dataset[\"Date\"] < six_months_ago][\"Date\"].unique():\n",
    "    if len(dataset[dataset[\"Date\"] ==  elem]) > 0:\n",
    "        all_daily_data.append(prepare_intra_day_data(elem))\n",
    "       \n",
    "all_daily_data = np.array(all_daily_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8719c45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_daily_data_val = []\n",
    "\n",
    "for elem in dataset[dataset[\"Date\"] >= six_months_ago][\"Date\"].unique():\n",
    "    if len(dataset[dataset[\"Date\"] ==  elem]) > 0:\n",
    "        all_daily_data_val.append(prepare_intra_day_data(elem))\n",
    "       \n",
    "all_daily_data_val = np.array(all_daily_data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba292b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findtarget(date):\n",
    "    close_day = df[df[\"Date\"] == date][\"Close\"].values[0]\n",
    "    df_intra_day = intradayndx_agg[intradayndx_agg[\"Date\"] == date]\n",
    "\n",
    "    index_value = df_intra_day.iloc[17][\"Close\"]\n",
    "\n",
    "    knockout_long = 0.93 * index_value\n",
    "    open_price_long = pricelong(index_value, knockout_long)\n",
    "    close_price_long = pricelong(close_day, knockout_long)\n",
    "\n",
    "    percentage_diff = ((close_price_long - open_price_long) / open_price_long) * 100\n",
    "\n",
    "    if percentage_diff <= -20:\n",
    "        return 0  # Loss > 20%\n",
    "    elif -20 < percentage_diff <= -10:\n",
    "        return 1  # Loss 10% to 20%\n",
    "    elif -10 < percentage_diff < 0:\n",
    "        return 2  # Loss 0% to 10%\n",
    "    elif 0 <= percentage_diff < 10:\n",
    "        return 3  # Win 0% to 10%\n",
    "    elif 10 <= percentage_diff <= 20:\n",
    "        return 4  # Win 10% to 20%\n",
    "    else:\n",
    "        return 5  # Win > 20%\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904c5c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#datetime.datetime(2024, 1, 24, 11, 53, 43, 403145)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83337cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "six_months_ago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887d007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_targets = []\n",
    "for elem in dataset[dataset[\"Date\"] < six_months_ago][\"Date\"].unique():\n",
    "    all_targets.append(findtarget(elem))\n",
    "all_targets = np.array(all_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c391413",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_targets_val = []\n",
    "for elem in dataset[dataset[\"Date\"] >= six_months_ago][\"Date\"].unique():\n",
    "    all_targets_val.append(findtarget(elem))\n",
    "all_targets_val = np.array(all_targets_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45fa3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b1b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_targets_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1877d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_targets[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43bcb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.bincount(all_targets)\n",
    "num_zeros = counts[0]\n",
    "num_ones = counts[1]\n",
    "num_twos = counts[2]\n",
    "num_threes = counts[3]\n",
    "num_fours = counts[4]\n",
    "num_fives = counts[5]\n",
    " \n",
    "\n",
    "# Calculate proportions\n",
    "total_elements = all_targets.size\n",
    "proportion_zeros = num_zeros / total_elements\n",
    "proportion_ones = num_ones / total_elements\n",
    "proportion_twos = num_twos / total_elements\n",
    "proportion_threes = num_threes / total_elements\n",
    "proportion_fours = num_fours / total_elements\n",
    "proportion_fives = num_fives / total_elements\n",
    " \n",
    "# Print the results\n",
    "print(\"Number of 0s:\", num_zeros)\n",
    "print(\"Number of 1s:\", num_ones)\n",
    "print(\"Number of 2s:\", num_twos)\n",
    "print(\"Number of 3s:\", num_threes)\n",
    "print(\"Number of 4s:\", num_fours)\n",
    "print(\"Number of 5s:\", num_fives)\n",
    " \n",
    "print(\"Proportion of 0s:\", proportion_zeros)\n",
    "print(\"Proportion of 1s:\", proportion_ones)\n",
    "print(\"Proportion of 2s:\", proportion_twos)\n",
    "print(\"Proportion of 3s:\", proportion_threes)\n",
    "print(\"Proportion of 4s:\", proportion_fours)\n",
    "print(\"Proportion of 5s:\", proportion_fives) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8e7d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.bincount(all_targets_val)\n",
    "num_zeros = counts[0]\n",
    "num_ones = counts[1]\n",
    "num_twos = counts[2]\n",
    "num_threes = counts[3]\n",
    "num_fours = counts[4]\n",
    " \n",
    " \n",
    "\n",
    "# Calculate proportions\n",
    "total_elements = all_targets_val.size\n",
    "proportion_zeros = num_zeros / total_elements\n",
    "proportion_ones = num_ones / total_elements\n",
    "proportion_twos = num_twos / total_elements\n",
    "proportion_threes = num_threes / total_elements\n",
    "proportion_fours = num_fours / total_elements\n",
    " \n",
    " \n",
    "# Print the results\n",
    "print(\"Number of 0s:\", num_zeros)\n",
    "print(\"Number of 1s:\", num_ones)\n",
    "print(\"Number of 2s:\", num_twos)\n",
    "print(\"Number of 3s:\", num_threes)\n",
    "print(\"Number of 4s:\", num_fours)\n",
    "print(\"Number of 5s:\", num_fives)\n",
    " \n",
    "print(\"Proportion of 0s:\", proportion_zeros)\n",
    "print(\"Proportion of 1s:\", proportion_ones)\n",
    "print(\"Proportion of 2s:\", proportion_twos)\n",
    "print(\"Proportion of 3s:\", proportion_threes)\n",
    "print(\"Proportion of 4s:\", proportion_fours)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cb41d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea03699b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124120f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "data = dataset[dataset[\"Date\"] < six_months_ago].values[:,1:]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d17375",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_val = dataset[dataset[\"Date\"] >= six_months_ago].values[:,1:]\n",
    "data_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3647dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = data\n",
    "X2 = all_daily_data[:,:]\n",
    "X2.shape\n",
    "num_rows, dim1, dim2 = X2.shape\n",
    "X2 = X2.reshape(num_rows, dim1 * dim2)\n",
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb91df4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_val = data_val\n",
    "X2_val = all_daily_data_val[:,:]\n",
    "\n",
    "num_rows, dim1, dim2 = X2_val.shape\n",
    "X2_val = X2_val.reshape(num_rows, dim1 * dim2)\n",
    "X2_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e862286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((X1, X2))\n",
    "\n",
    "print(X.shape)  # This should output (num, 176)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df80d2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.hstack((X1_val, X2_val))\n",
    "\n",
    "print(X_val.shape)  # This should output (num, 176)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8a2979",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = all_targets[:]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b1af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = all_targets_val[:]\n",
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd6dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78426b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc531230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1adc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e099587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8135f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split each array individually while keeping the same random_state to ensure matching indices\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y,random_state=42)\n",
    "data_train, data_test, _ , _ = train_test_split(dataset[dataset[\"Date\"] < six_months_ago], y, test_size=0.2, stratify=y,random_state=42)  # y is just to keep the split consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dedb954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9b784e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(np.float32)\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "y_test = y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900429c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all losses to 0 and all wins to 1 in y_train\n",
    "y_train = np.where(y_train < 3, 0, 1)\n",
    "y = np.where(y < 3, 0, 1)\n",
    "# Convert all losses to 0 and all wins to 1 in y_test\n",
    "y_test = np.where(y_test < 3, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7656f452",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(np.int64)\n",
    "\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "y_test = y_test.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c840f82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.sum() / len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b64fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.sum() / len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ae730",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = y_train.reshape(-1, 1)\n",
    "#y_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7824744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kan import *\n",
    "import torch\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94613ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_kan = {}\n",
    "dataset_kan['train_input'] = torch.from_numpy(X)\n",
    "dataset_kan['test_input'] = torch.from_numpy(X_test)\n",
    "dataset_kan['train_label'] = torch.from_numpy(y)\n",
    "dataset_kan['test_label'] = torch.from_numpy(y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6471a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57b23ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KAN(width=[176,16,2],  grid=3, k=3)\n",
    "\n",
    "def train_acc():\n",
    "    return torch.mean((torch.argmax(model(dataset_kan['train_input']), dim=1) == dataset_kan['train_label']).float())\n",
    "\n",
    "def test_acc():\n",
    "    return torch.mean((torch.argmax(model(dataset_kan['test_input']), dim=1) == dataset_kan['test_label']).float())\n",
    "\n",
    "results = model.train(dataset_kan, opt=\"LBFGS\",  steps=1, metrics=(train_acc,test_acc), loss_fn=torch.nn.CrossEntropyLoss())\n",
    "print(test_acc())\n",
    "print(train_acc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1a92f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = torch.argmax(model(torch.from_numpy(X_val.astype(np.float32))), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f2af16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_strategy(date, direction):\n",
    "    close_day = df[df[\"Date\"] == date][\"Close\"].values[0]\n",
    "    high_day = df[df[\"Date\"] == date][\"Close\"].values[0]\n",
    "    low_day = df[df[\"Date\"] == date][\"Close\"].values[0]\n",
    "    df_intra_day = intradayndx_agg[intradayndx_agg[\"Date\"]==date]\n",
    "\n",
    "    index_value = df_intra_day.iloc[17][\"Close\"]\n",
    "    \n",
    "\n",
    "    knockout_long = 0.96 * index_value\n",
    "    knockout_short = 1.04 * index_value\n",
    "\n",
    "    open_price_long = pricelong(index_value,knockout_long)\n",
    "    open_price_short = priceshort(index_value,knockout_short) \n",
    "     \n",
    "    close_price_long = pricelong(close_day,knockout_long)\n",
    "    close_price_short = priceshort(close_day,knockout_short)\n",
    "\n",
    "    if close_price_long < 0.71 *open_price_long:\n",
    "        close_price_long = 0.71 *open_price_long\n",
    "    if close_price_short < 0.71 *open_price_short:\n",
    "        close_price_short = 0.71 *open_price_short \n",
    "    \n",
    "    if direction == 1:\n",
    "        return 1+ (close_price_long - open_price_long) / open_price_long\n",
    "    else:\n",
    "        return 1+(close_price_short - open_price_short) / open_price_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b0f391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_strategy(date, direction):\n",
    "    close_day = df[df[\"Date\"] == date][\"Close\"].values[0]\n",
    "    high_day = df[df[\"Date\"] == date][\"Close\"].values[0]\n",
    "    low_day = df[df[\"Date\"] == date][\"Close\"].values[0]\n",
    "    df_intra_day = intradayndx_agg[intradayndx_agg[\"Date\"]==date]\n",
    "\n",
    "    index_value = df_intra_day.iloc[17][\"Close\"]\n",
    "    \n",
    "\n",
    "    knockout_long = 0.96 * index_value\n",
    "    knockout_short = 1.04 * index_value\n",
    "\n",
    "    open_price_long = pricelong(index_value,knockout_long)\n",
    "    open_price_short = priceshort(index_value,knockout_short) \n",
    "     \n",
    "    close_price_long = pricelong(close_day,knockout_long)\n",
    "    close_price_short = priceshort(close_day,knockout_short)\n",
    "\n",
    "    low_price_long_min = min([ pricelong(elem,knockout_long)  for elem in df_intra_day.iloc[18:][\"Close\"] ])\n",
    "    high_price_short_max = max([ priceshort(elem,knockout_short)  for elem in df_intra_day.iloc[18:][\"Close\"] ])\n",
    "    if close_price_long < 0.71 *open_price_long or low_price_long_min <   0.71 *open_price_long :\n",
    "        close_price_long = 0.71 *open_price_long\n",
    "    if close_price_short < 0.71 *open_price_short or high_price_short_max <   0.71 *open_price_long :\n",
    "        close_price_short = 0.71 *open_price_short \n",
    "    \n",
    "    if direction == 1:\n",
    "        return 1+ (close_price_long - open_price_long) / open_price_long\n",
    "    else:\n",
    "        return 1+(close_price_short - open_price_short) / open_price_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6636479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f32d06b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55be7d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val = dataset[dataset[\"Date\"] >=six_months_ago].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e478f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_performance = 1\n",
    "for i, elem in enumerate(dataset_val[:,0]):\n",
    "    direction = y_pred_class[i]\n",
    "    temp_performance = evaluate_strategy(elem, direction)\n",
    "    strat_performance *= temp_performance\n",
    "strat_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccda486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_strategy_stop(date, direction,stop):\n",
    "    close_day = df[df[\"Date\"] == date][\"Close\"].values[0]\n",
    "    high_day = df[df[\"Date\"] == date][\"Close\"].values[0]\n",
    "    low_day = df[df[\"Date\"] == date][\"Close\"].values[0]\n",
    "    df_intra_day = intradayndx_agg[intradayndx_agg[\"Date\"]==date]\n",
    "\n",
    "    index_value = df_intra_day.iloc[17][\"Close\"]\n",
    "    \n",
    "\n",
    "    knockout_long = 0.96 * index_value\n",
    "    knockout_short = 1.04 * index_value\n",
    "\n",
    "    open_price_long = pricelong(index_value,knockout_long)\n",
    "    open_price_short = priceshort(index_value,knockout_short) \n",
    "     \n",
    "    close_price_long = pricelong(close_day,knockout_long)\n",
    "    close_price_short = priceshort(close_day,knockout_short)\n",
    "\n",
    "    low_price_long_min = min([ pricelong(elem,knockout_long)  for elem in df_intra_day.iloc[18:][\"Close\"] ])\n",
    "    high_price_short_max = max([ priceshort(elem,knockout_short)  for elem in df_intra_day.iloc[18:][\"Close\"] ])\n",
    "    if close_price_long < stop*open_price_long or low_price_long_min <   stop *open_price_long :\n",
    "        close_price_long = stop *open_price_long\n",
    "    if close_price_short <stop *open_price_short or high_price_short_max <   stop *open_price_long :\n",
    "        close_price_short = stop *open_price_short \n",
    "    \n",
    "    if direction == 1:\n",
    "        return 1+ (close_price_long - open_price_long) / open_price_long\n",
    "    else:\n",
    "        return 1+(close_price_short - open_price_short) / open_price_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c5dc67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d08b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val = 7.160002074325383\n",
    "\n",
    " \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b47009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_kan(**params):\n",
    "    global best_val\n",
    "    layers = round(params[\"layers\"])\n",
    "    hidden_layers=[ int(params[\"x1\"]),int(params[\"x2\"]),int(params[\"x3\"]),int(params[\"x4\"])]\n",
    "   \n",
    "    epochs = int(params[\"epoch\"])\n",
    "    kan_struc = [176] + hidden_layers[0:layers] + [2]\n",
    "    #print(kan_struc)\n",
    "    model = KAN(width=kan_struc,  grid=3, k=3)\n",
    "    try:\n",
    "        results = model.train(dataset_kan, opt=\"LBFGS\",  steps=epochs, metrics=(train_acc, test_acc), loss_fn=torch.nn.CrossEntropyLoss())\n",
    "    except:\n",
    "        return 0\n",
    "    y_pred_class = torch.argmax(model(torch.from_numpy(X_val.astype(np.float32))), dim=1)\n",
    "\n",
    "    strat_performance = 1\n",
    "    stats = []\n",
    "    for i, elem in enumerate(dataset_val[:,0]):\n",
    "        direction = y_pred_class[i]\n",
    "        temp_performance = evaluate_strategy_stop(elem, direction,0.93)\n",
    "        strat_performance *= temp_performance\n",
    "        stats.append([elem,temp_performance,strat_performance,direction])\n",
    "    if strat_performance > best_val:\n",
    "        best_val = strat_performance\n",
    "        model.save_ckpt('bestmodel_kan')\n",
    "        file_path = 'kan_struc_best.json'\n",
    "\n",
    "        df_pic = pd.DataFrame(stats, columns=['Date', 'Day_Performance', 'Current_Performance','Direction'])\n",
    "\n",
    "        # Create the plot\n",
    "        fig = go.Figure()\n",
    "\n",
    "        # Add traces for each value column\n",
    "        fig.add_trace(go.Scatter(x=df_pic['Date'], y=df_pic['Day_Performance'], mode='lines+markers', name='Day_Performance'))\n",
    "        fig.add_trace(go.Scatter(x=df_pic['Date'], y=df_pic['Current_Performance'], mode='lines+markers', name='Current_Performance'))\n",
    "\n",
    "    # Update layout\n",
    "        fig.update_layout(\n",
    "        title='Time Series Data',  \n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Performance',\n",
    "        template='plotly'\n",
    "    )\n",
    "\n",
    "    # Show the plot\n",
    "        fig.show()\n",
    "\n",
    "        with open(file_path, 'w') as json_file:\n",
    "            json.dump(kan_struc, json_file)\n",
    "\n",
    "    return strat_performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c407ac78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import boto3\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.util import UtilityFunction\n",
    "\n",
    "# Define the S3 bucket and paths\n",
    "BUCKET_NAME = 'REDACTED_BUCKET'\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Early stopping variables\n",
    "no_improvement_count = 0\n",
    "best_val = 7.160002074325383\n",
    "wait = 50  # Early stopping wait iterations\n",
    "def simple_kan(**params):\n",
    "    global best_val\n",
    "    layers = round(params[\"layers\"])\n",
    "    hidden_layers=[int(params[\"x1\"]), int(params[\"x2\"]), int(params[\"x3\"]), int(params[\"x4\"])]\n",
    "    epochs = int(params[\"epoch\"])\n",
    "    kan_struc = [176] + hidden_layers[0:layers] + [2]\n",
    "\n",
    "    model = KAN(width=kan_struc, grid=3, k=3)\n",
    "    \n",
    "    try:\n",
    "        results = model.train(dataset_kan, opt=\"LBFGS\", steps=epochs, metrics=(train_acc, test_acc), loss_fn=torch.nn.CrossEntropyLoss())\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "    y_pred_class = torch.argmax(model(torch.from_numpy(X_val.astype(np.float32))), dim=1)\n",
    "\n",
    "    strat_performance = 1\n",
    "    stats = []\n",
    "    for i, elem in enumerate(dataset_val[:, 0]):\n",
    "        direction = y_pred_class[i]\n",
    "        temp_performance = evaluate_strategy_stop(elem, direction, 0.93)\n",
    "        strat_performance *= temp_performance\n",
    "        stats.append([elem, temp_performance, strat_performance, direction])\n",
    "\n",
    "    if strat_performance > best_val:\n",
    "        \n",
    "        model.save_ckpt('bestmodel_kan')\n",
    "        \n",
    "        # Save KAN structure as JSON\n",
    "        file_path = 'kan_struc_best.json'\n",
    "        with open(file_path, 'w') as json_file:\n",
    "            json.dump(kan_struc, json_file)\n",
    "        \n",
    "        # Upload KAN structure to S3 bucket\n",
    "        s3_client.upload_file(file_path, BUCKET_NAME, file_path)\n",
    "        \n",
    "        # Upload model weights to S3 bucket\n",
    "        model_weight_path = 'model_ckpt/bestmodel_kan'\n",
    "        s3_client.upload_file(model_weight_path, BUCKET_NAME,\"bestmodel_kan\")\n",
    "\n",
    "        # Prepare data for Plotly\n",
    "        df_pic = pd.DataFrame(stats, columns=['Date', 'Day_Performance', 'Current_Performance', 'Direction'])\n",
    "\n",
    "        # Create the plot with dark mode\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(x=df_pic['Date'], y=df_pic['Day_Performance'], mode='lines+markers', name='Day_Performance'))\n",
    "        fig.add_trace(go.Scatter(x=df_pic['Date'], y=df_pic['Current_Performance'], mode='lines+markers', name='Current_Performance'))\n",
    "\n",
    "        fig.update_layout(\n",
    "            title='Time Series Data',\n",
    "            xaxis_title='Date',\n",
    "            yaxis_title='Performance',\n",
    "            template='plotly_dark'  # Dark mode enabled\n",
    "        )\n",
    "\n",
    "        # Show the plot\n",
    "        fig.show()\n",
    "\n",
    "    return strat_performance\n",
    "\n",
    "# Initialize Bayesian optimization with utility function\n",
    "utility = UtilityFunction(kind=\"ucb\", kappa=2.576, xi=0.0)\n",
    "optimizer = BayesianOptimization(\n",
    "    f=simple_kan,\n",
    "    pbounds={'x1': (1, 32), 'x2': (1, 32), 'x3': (1, 32), 'x4': (1, 32),\n",
    "             'epoch': (2, 50), 'layers': (0.5001, 4.5)},\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Set Gaussian process parameters\n",
    "optimizer.set_gp_params(alpha=1e-6)\n",
    "\n",
    "# Run the Bayesian optimization\n",
    "TOTAL_ITERATIONS = 150\n",
    "#optimizer.maximize(init_points=5, n_iter=0)  # Run initial exploration for 5 points\n",
    "\"\"\"\n",
    "for i in tqdm(range(TOTAL_ITERATIONS - 5)):  # Subtract the initial 5 points\n",
    "    # Perform the next optimization step\n",
    "    suggested_params = optimizer.suggest(utility)\n",
    "    target = simple_kan(**suggested_params)  # Get target for the suggested parameters\n",
    "    optimizer.register(params=suggested_params, target=target)  # Register new observation\n",
    "    \n",
    "    # Check for early stopping\n",
    "    if target > best_val:\n",
    "        best_val = target\n",
    "        no_improvement_count = 0\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "\n",
    "    if no_improvement_count >= wait:\n",
    "        print(f\"Early stopping triggered after {i + 1 + 5} iterations\")  # +5 accounts for the initial points\n",
    "        break\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0998662",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_ITERATIONS = 150\n",
    "optimizer = BayesianOptimization(\n",
    "    f=simple_kan,\n",
    "     pbounds={'x1': (1, 32),'x2': (1, 32),'x3': (1, 32),'x4': (1, 32),\n",
    " \n",
    " 'epoch': (2, 50),\n",
    " 'layers': (0.5001, 4.5)},\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    #bounds_transformer=bounds_transformer\n",
    ")\n",
    "\n",
    "#optimizer.maximize(\n",
    " #  init_points=5,\n",
    " ##   n_iter=TOTAL_ITERATIONS-5,\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44edd86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_kan():\n",
    "    file_path = 'kan_struc_best.json'\n",
    "\n",
    "    # Load the JSON file\n",
    "    with open(file_path, 'r') as file:\n",
    "        kan_struc = json.load(file)\n",
    "    model = KAN(width=kan_struc,  grid=3, k=3)\n",
    "    print(kan_struc)\n",
    "    model.load_ckpt('bestmodel_kan')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845cd0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af749aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_kan()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f06077",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = torch.argmax(model(torch.from_numpy(X_val.astype(np.float32))), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8be359",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_performance = 1\n",
    "stats = []\n",
    "for i, elem in enumerate(dataset_val[:,0]):\n",
    "    direction = y_pred_class[i]\n",
    "    temp_performance = evaluate_strategy_stop(elem, direction, 0.93)\n",
    "    \n",
    "    \n",
    "    strat_performance *= temp_performance\n",
    "    stats.append([elem,temp_performance,strat_performance,direction])\n",
    "strat_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976afd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046efe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_val[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81deb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6402f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Assuming stats is already loaded\n",
    "df_pic = pd.DataFrame(stats, columns=['Date', 'Day_Performance', 'Current_Performance', 'Direction'])\n",
    "\n",
    "# Convert 'Date' to datetime to manage future date calculations\n",
    "df_pic['Date'] = pd.to_datetime(df_pic['Date'])\n",
    "\n",
    "# Define the future date you want to predict for\n",
    "future_date_str = '2025-01-31'  # Example: 'YYYY-MM-DD'\n",
    "future_date = pd.to_datetime(future_date_str)\n",
    "\n",
    "# Convert dates to numeric values (days since first date) for fitting\n",
    "base_date = df_pic['Date'].min()\n",
    "days_since_start = (df_pic['Date'] - base_date).dt.days\n",
    "\n",
    "# Fit polynomial using days instead of index\n",
    "z = np.polyfit(days_since_start, df_pic['Current_Performance'], 1)\n",
    "p = np.poly1d(z)\n",
    "\n",
    "# Create the original trendline using days\n",
    "trendline_values = p(days_since_start)\n",
    "\n",
    "# Calculate days until future date for prediction\n",
    "days_to_future = (future_date - base_date).days\n",
    "future_value = p(days_to_future)\n",
    "\n",
    "# Create the plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces for each value column\n",
    "fig.add_trace(go.Scatter(x=df_pic['Date'], y=df_pic['Day_Performance'], mode='lines+markers', name='Day_Performance'))\n",
    "fig.add_trace(go.Scatter(x=df_pic['Date'], y=df_pic['Current_Performance'], mode='lines+markers', name='Current_Performance'))\n",
    "\n",
    "# Add the trendline for the original data\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_pic['Date'],\n",
    "    y=trendline_values,\n",
    "    mode='lines',\n",
    "    name='Trendline (Current Performance)',\n",
    "    line=dict(dash='dot')\n",
    "))\n",
    "\n",
    "# Add the future prediction point\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[future_date],\n",
    "    y=[future_value],\n",
    "    mode='markers+text',\n",
    "    name='Predicted Value',\n",
    "    text=[f'Predicted: {future_value:.2f}'],\n",
    "    textposition='top center',\n",
    "    marker=dict(color='red', size=10)\n",
    "))\n",
    "\n",
    "# Update layout with dark theme\n",
    "fig.update_layout(\n",
    "    title='Time Series Data with Future Prediction',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Performance',\n",
    "    template='plotly_dark'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f08629",
   "metadata": {},
   "outputs": [],
   "source": [
    "40.78/31.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723884f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First, let's add the streak column\n",
    "def calculate_streak(df):\n",
    "    streak = 0\n",
    "    streaks = []\n",
    "    \n",
    "    for performance in df['Day_Performance']:\n",
    "        if performance > 1:\n",
    "            streak += 1\n",
    "        elif performance < 1:\n",
    "            streak -= 1\n",
    "        # if performance is 0, streak remains unchanged\n",
    "        streaks.append(streak)\n",
    "    \n",
    "    return streaks\n",
    "\n",
    "# Add the streak column to your dataframe\n",
    "df_pic['streak'] = calculate_streak(df_pic)\n",
    "# Create the plot with dark theme\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add trace for streak\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=df_pic['Date'], \n",
    "        y=df_pic['streak'], \n",
    "        mode='lines+markers', \n",
    "        name='streak',\n",
    "        line=dict(width=2),\n",
    "        marker=dict(size=6)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update layout with dark theme\n",
    "fig.update_layout(\n",
    "    title='Time Series Data for ML Performance',\n",
    "    xaxis_title='Date', \n",
    "    yaxis_title='Performance',\n",
    "    template='plotly_dark',  # Changed to dark theme\n",
    "    paper_bgcolor='rgba(0,0,0,1)',  # Dark background\n",
    "    plot_bgcolor='rgba(0,0,0,1)',   # Dark plot area\n",
    "    font=dict(color='white'),       # White text\n",
    "    xaxis=dict(\n",
    "        showgrid=True,\n",
    "        gridwidth=1,\n",
    "        gridcolor='rgba(128,128,128,0.2)',\n",
    "        zeroline=True,\n",
    "        zerolinewidth=1,\n",
    "        zerolinecolor='rgba(128,128,128,0.2)'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        showgrid=True,\n",
    "        gridwidth=1,\n",
    "        gridcolor='rgba(128,128,128,0.2)',\n",
    "        zeroline=True,\n",
    "        zerolinewidth=1,\n",
    "        zerolinecolor='rgba(128,128,128,0.2)'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b500929",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae040ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix naming conventions for clarity\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_train_final, X_val_final, _, _ = train_test_split(\n",
    "    dataset[dataset[\"Date\"] < six_months_ago].values, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Print to verify shapes\n",
    "print(f\"Train data shape: {X_train_tensor.shape}, Labels shape: {y_train_tensor.shape}\")\n",
    "print(f\"Validation data shape: {X_val_tensor.shape}, Labels shape: {y_val_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e426d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(X_val_final) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d080ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1371f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model and data parameters\n",
    "num_features = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e6f8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3dbe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_validation_predictions(model, val_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations\n",
    "        for inputs, _ in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted_classes = torch.max(outputs, 1)  # Get the class with the highest score\n",
    "            all_predictions.append(predicted_classes)\n",
    "\n",
    "    # Combine all predictions into a single tensor\n",
    "    all_predictions = torch.cat(all_predictions)\n",
    "\n",
    "    return all_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc272753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from bayes_opt import BayesianOptimization\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Device handling: Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "best_performance = -float('inf')\n",
    "\n",
    "# Updated ComplexNN with different dropout rates\n",
    "class ComplexNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, hidden_units_1=128, hidden_units_2=64, hidden_units_3=32,\n",
    "                 dropout_rate_1=0.3, dropout_rate_2=0.3, dropout_rate_3=0.3):\n",
    "        super(ComplexNN, self).__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_units_1),\n",
    "            nn.BatchNorm1d(hidden_units_1),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate_1),\n",
    "            nn.Linear(hidden_units_1, hidden_units_2),\n",
    "            nn.BatchNorm1d(hidden_units_2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate_2),\n",
    "            nn.Linear(hidden_units_2, hidden_units_3),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate_3)\n",
    "        )\n",
    "        self.head = nn.Linear(hidden_units_3, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shared(x)\n",
    "        return self.head(x)\n",
    "\n",
    "\n",
    "# Updated train_and_evaluate\n",
    "def train_and_evaluate(lr, weight_decay, hidden_units_1, hidden_units_2, hidden_units_3, dropout_rate_1, dropout_rate_2, dropout_rate_3):\n",
    "    global best_performance\n",
    "    model = ComplexNN(\n",
    "        input_size=num_features,\n",
    "        num_classes=num_classes,\n",
    "        hidden_units_1=int(hidden_units_1),\n",
    "        hidden_units_2=int(hidden_units_2),\n",
    "        hidden_units_3=int(hidden_units_3),\n",
    "        dropout_rate_1=dropout_rate_1,\n",
    "        dropout_rate_2=dropout_rate_2,\n",
    "        dropout_rate_3=dropout_rate_3\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "    num_epochs = 50\n",
    "    patience = 10\n",
    "    best_loss = float('inf')\n",
    "    wait = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                break\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # Evaluate the strategy\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(torch.from_numpy(X_val.astype(np.float32)).to(device))\n",
    "        _, predicted_classes = torch.max(outputs, 1)\n",
    "\n",
    "    strat_performance = 1\n",
    "    for i, elem in enumerate(dataset_val[:, 0]):\n",
    "        direction = predicted_classes[i].item()\n",
    "        temp_performance = evaluate_strategy_stop(elem, direction, 0.93)\n",
    "        strat_performance *= temp_performance\n",
    "\n",
    "    # Check for the best performance\n",
    "    if strat_performance > best_performance:\n",
    "        best_performance = strat_performance\n",
    "        torch.save(model.state_dict(), 'best_model_state.pth')\n",
    "        print(\"New best performance:\", best_performance)\n",
    "        print(\"Model saved.\")\n",
    "\n",
    "        # Plot training and validation losses\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(y=train_losses, mode='lines+markers', name='Training Loss'))\n",
    "        fig.add_trace(go.Scatter(y=val_losses, mode='lines+markers', name='Validation Loss'))\n",
    "        fig.update_layout(\n",
    "            title=\"Training and Validation Loss\",\n",
    "            xaxis_title=\"Epochs\",\n",
    "            yaxis_title=\"Loss\",\n",
    "            template=\"plotly_dark\",\n",
    "            legend=dict(x=0, y=1)\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "    return strat_performance\n",
    "\n",
    "\n",
    "# Updated Bayesian Optimization\n",
    "optimizer_params = {\n",
    "    'lr': (1e-4, 1e-2),\n",
    "    'weight_decay': (1e-5, 1e-2),\n",
    "    'hidden_units_1': (64, 256),\n",
    "    'hidden_units_2': (32, 128),\n",
    "    'hidden_units_3': (16, 64),\n",
    "    'dropout_rate_1': (0.1, 0.5),\n",
    "    'dropout_rate_2': (0.1, 0.5),\n",
    "    'dropout_rate_3': (0.1, 0.5)\n",
    "}\n",
    "\n",
    "bo = BayesianOptimization(\n",
    "    f=train_and_evaluate,\n",
    "    pbounds=optimizer_params,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "bo.maximize(init_points=5, n_iter=100)\n",
    "print(\"Best parameters found:\", bo.max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1716bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c5daa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f446158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a loading method to load the bp.max into the above defined pytorch class as a model\n",
    "bo.max['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28f77dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Initialize the model using the best parameters found during Bayesian Optimization\n",
    "model = ComplexNN(\n",
    "    input_size=num_features,\n",
    "    num_classes=num_classes,\n",
    "    hidden_units_1=int(bo.max['params']['hidden_units_1']),\n",
    "    hidden_units_2=int(bo.max['params']['hidden_units_2']),\n",
    "    hidden_units_3=int(bo.max['params']['hidden_units_3']),\n",
    "    dropout_rate_1=bo.max['params']['dropout_rate_1'],  # Different dropout for first layer\n",
    "    dropout_rate_2=bo.max['params']['dropout_rate_2'],  # Different dropout for second layer\n",
    "    dropout_rate_3=bo.max['params']['dropout_rate_3']   # Different dropout for third layer\n",
    ")\n",
    "\n",
    "# Load the model state\n",
    "model.load_state_dict(torch.load('best_model_state.pth'))\n",
    "\n",
    "# Ensure model is on the correct device (GPU or CPU)\n",
    " \n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully and set to evaluation mode!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e8f09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0784e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class = torch.argmax(model(torch.from_numpy(X_val.astype(np.float32))), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69ba49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "y_pred_class = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculations\n",
    "        \n",
    "        outputs = model(torch.from_numpy(X_val.astype(np.float32)))\n",
    "        _, predicted_classes = torch.max(outputs, 1)  # Get the class with the highest score\n",
    "        y_pred_class.append(predicted_classes)\n",
    "\n",
    "# Combine all predictions into a single tensor\n",
    "y_pred_class = torch.cat(y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc576c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270c003f",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_performance = 1\n",
    "stats = []\n",
    "for i, elem in enumerate(dataset_val[:,0]):\n",
    "    direction = y_pred_class[i]\n",
    "    temp_performance = evaluate_strategy_stop(elem, direction, 0.93)\n",
    "    \n",
    "    \n",
    "    strat_performance *= temp_performance\n",
    "    stats.append([elem,temp_performance,strat_performance,direction])\n",
    "strat_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe38194",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming stats is already loaded\n",
    "df_pic = pd.DataFrame(stats, columns=['Date', 'Day_Performance', 'Current_Performance', 'Direction'])\n",
    "\n",
    "# Convert 'Date' to datetime to manage future date calculations\n",
    "df_pic['Date'] = pd.to_datetime(df_pic['Date'])\n",
    "\n",
    "# Define the future date you want to predict for\n",
    "future_date_str = '2025-01-31'  # Example: 'YYYY-MM-DD'\n",
    "future_date = pd.to_datetime(future_date_str)\n",
    "\n",
    "# Convert dates to numeric values (days since first date) for fitting\n",
    "base_date = df_pic['Date'].min()\n",
    "days_since_start = (df_pic['Date'] - base_date).dt.days\n",
    "\n",
    "# Fit polynomial using days instead of index\n",
    "z = np.polyfit(days_since_start, df_pic['Current_Performance'], 1)\n",
    "p = np.poly1d(z)\n",
    "\n",
    "# Create the original trendline using days\n",
    "trendline_values = p(days_since_start)\n",
    "\n",
    "# Calculate days until future date for prediction\n",
    "days_to_future = (future_date - base_date).days\n",
    "future_value = p(days_to_future)\n",
    "\n",
    "# Create the plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces for each value column\n",
    "fig.add_trace(go.Scatter(x=df_pic['Date'], y=df_pic['Day_Performance'], mode='lines+markers', name='Day_Performance'))\n",
    "fig.add_trace(go.Scatter(x=df_pic['Date'], y=df_pic['Current_Performance'], mode='lines+markers', name='Current_Performance'))\n",
    "\n",
    "# Add the trendline for the original data\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_pic['Date'],\n",
    "    y=trendline_values,\n",
    "    mode='lines',\n",
    "    name='Trendline (Current Performance)',\n",
    "    line=dict(dash='dot')\n",
    "))\n",
    "\n",
    "# Add the future prediction point\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[future_date],\n",
    "    y=[future_value],\n",
    "    mode='markers+text',\n",
    "    name='Predicted Value',\n",
    "    text=[f'Predicted: {future_value:.2f}'],\n",
    "    textposition='top center',\n",
    "    marker=dict(color='red', size=10)\n",
    "))\n",
    "\n",
    "# Update layout with dark theme\n",
    "fig.update_layout(\n",
    "    title='Time Series Data with Future Prediction',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Performance',\n",
    "    template='plotly_dark'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5703cbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicComplexNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, num_hidden_layers=3, hidden_units=None, dropout_rates=None):\n",
    "        \"\"\"\n",
    "        input_size: int, number of input features\n",
    "        num_classes: int, number of output classes\n",
    "        num_hidden_layers: int, number of hidden layers\n",
    "        hidden_units: list of int, sizes of each hidden layer\n",
    "        dropout_rates: list of float, dropout rates for each layer\n",
    "        \"\"\"\n",
    "        super(DynamicComplexNN, self).__init__()\n",
    "\n",
    "        if hidden_units is None or dropout_rates is None:\n",
    "            raise ValueError(\"hidden_units and dropout_rates must be provided as lists\")\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        current_input_size = input_size\n",
    "\n",
    "        # Dynamically create layers\n",
    "        for i in range(num_hidden_layers):\n",
    "            self.layers.append(nn.Linear(current_input_size, hidden_units[i]))\n",
    "            self.layers.append(nn.BatchNorm1d(hidden_units[i]))\n",
    "            self.layers.append(nn.GELU())  # Activation\n",
    "            self.layers.append(nn.Dropout(dropout_rates[i]))  # Dropout\n",
    "            current_input_size = hidden_units[i]\n",
    "\n",
    "        # Final output layer\n",
    "        self.output = nn.Linear(current_input_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.output(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5a0d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated train_and_evaluate\n",
    "def train_and_evaluate(lr, weight_decay, num_hidden_layers, *hidden_units_and_dropouts):    \n",
    "    global best_performance\n",
    "    \n",
    "\n",
    "    # Extract hidden units and dropout rates dynamically\n",
    "    num_hidden_layers = int(num_hidden_layers)\n",
    "    hidden_units = []\n",
    "    dropout_rates = []\n",
    "\n",
    "    for i in range(num_hidden_layers):\n",
    "        hidden_units.append(int(hidden_units_and_dropouts[2 * i]))      # Even indices: Hidden units\n",
    "        dropout_rates.append(hidden_units_and_dropouts[2 * i + 1])     # Odd indices: Dropout rates\n",
    "\n",
    "    # Initialize the model\n",
    "    model = DynamicComplexNN(\n",
    "        input_size=num_features,\n",
    "        num_classes=num_classes,\n",
    "        num_hidden_layers=num_hidden_layers,\n",
    "        hidden_units=hidden_units,\n",
    "        dropout_rates=dropout_rates\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "    num_epochs = 50\n",
    "    patience = 10\n",
    "    best_loss = float('inf')\n",
    "    wait = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                break\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # Evaluate the strategy\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(torch.from_numpy(X_val.astype(np.float32)).to(device))\n",
    "        _, predicted_classes = torch.max(outputs, 1)\n",
    "\n",
    "    strat_performance = 1\n",
    "    for i, elem in enumerate(dataset_val[:, 0]):\n",
    "        direction = predicted_classes[i].item()\n",
    "        temp_performance = evaluate_strategy_stop(elem, direction, 0.93)\n",
    "        strat_performance *= temp_performance\n",
    "\n",
    "    # Check for the best performance\n",
    "    if strat_performance > best_performance:\n",
    "        best_performance = strat_performance\n",
    "        torch.save(model.state_dict(), 'best_model_weights.pth')\n",
    "        print(\"New best performance:\", best_performance)\n",
    "        print(\"Model saved.\")\n",
    "\n",
    "        # Plot training and validation losses\n",
    "        fig = go.Figure()\n",
    "        fig.add_trace(go.Scatter(y=train_losses, mode='lines+markers', name='Training Loss'))\n",
    "        fig.add_trace(go.Scatter(y=val_losses, mode='lines+markers', name='Validation Loss'))\n",
    "        fig.update_layout(\n",
    "            title=\"Training and Validation Loss\",\n",
    "            xaxis_title=\"Epochs\",\n",
    "            yaxis_title=\"Loss\",\n",
    "            template=\"plotly_dark\",\n",
    "            legend=dict(x=0, y=1)\n",
    "        )\n",
    "        fig.show()\n",
    "\n",
    "    return strat_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9acf6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_performance = -float('inf')\n",
    "\n",
    "max_layers = 20  # Allow up to 5 layers dynamically\n",
    "optimizer_params = {\n",
    "    'lr': (1e-4, 1e-2),\n",
    "    'weight_decay': (1e-5, 1e-2),\n",
    "    'num_hidden_layers': (1, max_layers),  # Number of layers: 1 to max_layers\n",
    "}\n",
    "\n",
    "# Add hidden units and dropout rates dynamically\n",
    "for i in range(max_layers):\n",
    "    optimizer_params[f'hidden_units_{i+1}'] = (2, 512)  # Hidden units range\n",
    "    optimizer_params[f'dropout_rate_{i+1}'] = (0.01, 0.5)  # Dropout rate range\n",
    "\n",
    "\n",
    "# Define the wrapper for train_and_evaluate\n",
    "def bayesian_eval_wrapper(lr, weight_decay, num_hidden_layers, **kwargs):\n",
    "    num_hidden_layers = int(num_hidden_layers)\n",
    "    hidden_units_and_dropouts = []\n",
    "\n",
    "    # Extract hidden units and dropout rates dynamically\n",
    "    for i in range(num_hidden_layers):\n",
    "        hidden_units_and_dropouts.append(kwargs[f\"hidden_units_{i+1}\"])\n",
    "        hidden_units_and_dropouts.append(kwargs[f\"dropout_rate_{i+1}\"])\n",
    "\n",
    "    return train_and_evaluate(lr, weight_decay, num_hidden_layers, *hidden_units_and_dropouts)\n",
    "\n",
    "# Run Bayesian Optimization\n",
    "bo = BayesianOptimization(\n",
    "    f=bayesian_eval_wrapper,\n",
    "    pbounds=optimizer_params,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "bo.maximize(init_points=5, n_iter=1500)\n",
    "print(\"Best parameters found:\", bo.max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6d212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch\n",
    "# Extract the best parameters from Bayesian Optimization\n",
    "best_params = bo.max['params']\n",
    "\n",
    "# Extract key parameters\n",
    "num_hidden_layers = int(best_params['num_hidden_layers'])\n",
    "hidden_units = []\n",
    "dropout_rates = []\n",
    "\n",
    "# Dynamically extract hidden units and dropout rates\n",
    "for i in range(num_hidden_layers):\n",
    "    hidden_units.append(int(best_params[f'hidden_units_{i+1}']))\n",
    "    dropout_rates.append(best_params[f'dropout_rate_{i+1}'])\n",
    "\n",
    "# Initialize the model with the dynamic architecture\n",
    "model = DynamicComplexNN(\n",
    "    input_size=num_features,\n",
    "    num_classes=num_classes,\n",
    "    num_hidden_layers=num_hidden_layers,\n",
    "    hidden_units=hidden_units,\n",
    "    dropout_rates=dropout_rates\n",
    ")\n",
    " \n",
    "\n",
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load('best_model_weights.pth', map_location=device))\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Print confirmation and model structure\n",
    "print(\"Model loaded successfully with the following architecture:\")\n",
    "print(f\"Number of Hidden Layers: {num_hidden_layers}\")\n",
    "print(f\"Hidden Units: {hidden_units}\")\n",
    "print(f\"Dropout Rates: {dropout_rates}\")\n",
    "print(model)\n",
    "\n",
    "y_pred_class = []\n",
    "\n",
    "with torch.no_grad():  # Disable gradient calculations\n",
    "        \n",
    "        outputs = model(torch.from_numpy(X_val.astype(np.float32)))\n",
    "        predicted_classes =  torch.argmax(outputs, dim=1)  # Get the class with the highest score\n",
    "        y_pred_class.append(predicted_classes)\n",
    "\n",
    "# Combine all predictions into a single tensor\n",
    "y_pred_class = torch.cat(y_pred_class)\n",
    "strat_performance = 1\n",
    "stats = []\n",
    "for i, elem in enumerate(dataset_val[:,0]):\n",
    "    direction = y_pred_class[i]\n",
    "    print(direction)\n",
    "    temp_performance = evaluate_strategy_stop(elem, direction, 0.93)\n",
    "    \n",
    "    \n",
    "    strat_performance *= temp_performance\n",
    "    stats.append([elem,temp_performance,strat_performance,direction])\n",
    "\n",
    "# Assuming stats is already loaded\n",
    "df_pic = pd.DataFrame(stats, columns=['Date', 'Day_Performance', 'Current_Performance', 'Direction'])\n",
    "\n",
    "# Convert 'Date' to datetime to manage future date calculations\n",
    "df_pic['Date'] = pd.to_datetime(df_pic['Date'])\n",
    "\n",
    "# Define the future date you want to predict for\n",
    "future_date_str = '2025-01-31'  # Example: 'YYYY-MM-DD'\n",
    "future_date = pd.to_datetime(future_date_str)\n",
    "\n",
    "# Convert dates to numeric values (days since first date) for fitting\n",
    "base_date = df_pic['Date'].min()\n",
    "days_since_start = (df_pic['Date'] - base_date).dt.days\n",
    "\n",
    "# Fit polynomial using days instead of index\n",
    "z = np.polyfit(days_since_start, df_pic['Current_Performance'], 1)\n",
    "p = np.poly1d(z)\n",
    "\n",
    "# Create the original trendline using days\n",
    "trendline_values = p(days_since_start)\n",
    "\n",
    "# Calculate days until future date for prediction\n",
    "days_to_future = (future_date - base_date).days\n",
    "future_value = p(days_to_future)\n",
    "\n",
    "# Create the plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add traces for each value column\n",
    "fig.add_trace(go.Scatter(x=df_pic['Date'], y=df_pic['Day_Performance'], mode='lines+markers', name='Day_Performance'))\n",
    "fig.add_trace(go.Scatter(x=df_pic['Date'], y=df_pic['Current_Performance'], mode='lines+markers', name='Current_Performance'))\n",
    "\n",
    "# Add the trendline for the original data\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_pic['Date'],\n",
    "    y=trendline_values,\n",
    "    mode='lines',\n",
    "    name='Trendline (Current Performance)',\n",
    "    line=dict(dash='dot')\n",
    "))\n",
    "\n",
    "# Add the future prediction point\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=[future_date],\n",
    "    y=[future_value],\n",
    "    mode='markers+text',\n",
    "    name='Predicted Value',\n",
    "    text=[f'Predicted: {future_value:.2f}'],\n",
    "    textposition='top center',\n",
    "    marker=dict(color='red', size=10)\n",
    "))\n",
    "\n",
    "# Update layout with dark theme\n",
    "fig.update_layout(\n",
    "    title='Time Series Data with Future Prediction',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Performance',\n",
    "    template='plotly_dark'\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd25764",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "  \"wait\": 0,\n",
    "  \"counter\": 0,\n",
    "  \"X1\": [\n",
    "    [\n",
    "      -0.13192251324653625,\n",
    "      -0.18630719184875488,\n",
    "      -0.011558888480067253,\n",
    "      -0.019972870126366615,\n",
    "      -0.008799443952739239,\n",
    "      -0.022233862429857254,\n",
    "      -0.0105429133400321,\n",
    "      -0.010949401184916496,\n",
    "      -0.0044204252772033215,\n",
    "      -0.015692589804530144,\n",
    "      -0.027859758585691452,\n",
    "      -0.028866613283753395,\n",
    "      -0.022564474493265152,\n",
    "      -0.03436919301748276,\n",
    "      -0.034305281937122345,\n",
    "      -0.038298334926366806,\n",
    "      -0.02899443916976452,\n",
    "      -0.043828755617141724,\n",
    "      -0.013844897039234638,\n",
    "      0.07633326202630997,\n",
    "      0.21024826169013977,\n",
    "      0.18406324088573456,\n",
    "      1,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      1,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      1,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      1,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      0,\n",
    "      1\n",
    "    ]\n",
    "  ],\n",
    "  \"X2\": [\n",
    "    [\n",
    "      [\n",
    "        0.01715690828859806,\n",
    "        -0.01503333356231451,\n",
    "        0.0005264173378236592,\n",
    "        -0.01715690828859806,\n",
    "        0.01503333356231451,\n",
    "        -0.0005264173378236592\n",
    "      ],\n",
    "      [\n",
    "        0.12117267400026321,\n",
    "        -0.01503333356231451,\n",
    "        0.12117267400026321,\n",
    "        -0.12117267400026321,\n",
    "        0.01503333356231451,\n",
    "        -0.12117267400026321\n",
    "      ],\n",
    "      [\n",
    "        0.12117267400026321,\n",
    "        -0.01503333356231451,\n",
    "        0.07095891237258911,\n",
    "        -0.12117267400026321,\n",
    "        0.01503333356231451,\n",
    "        -0.07095891237258911\n",
    "      ],\n",
    "      [\n",
    "        0.12117267400026321,\n",
    "        -0.018070796504616737,\n",
    "        0.011233698576688766,\n",
    "        -0.12117267400026321,\n",
    "        0.018070796504616737,\n",
    "        -0.011233698576688766\n",
    "      ],\n",
    "      [\n",
    "        0.12117267400026321,\n",
    "        -0.018070796504616737,\n",
    "        0.12019611895084381,\n",
    "        -0.12117267400026321,\n",
    "        0.018070796504616737,\n",
    "        -0.12019611895084381\n",
    "      ],\n",
    "      [\n",
    "        0.12117267400026321,\n",
    "        -0.018070796504616737,\n",
    "        0.06906571984291077,\n",
    "        -0.12117267400026321,\n",
    "        0.018070796504616737,\n",
    "        -0.06906571984291077\n",
    "      ],\n",
    "      [\n",
    "        0.12117267400026321,\n",
    "        -0.018070796504616737,\n",
    "        0.007597718387842178,\n",
    "        -0.12117267400026321,\n",
    "        0.018070796504616737,\n",
    "        -0.007597718387842178\n",
    "      ],\n",
    "      [\n",
    "        0.12117267400026321,\n",
    "        -0.03401527553796768,\n",
    "        0.04343205690383911,\n",
    "        -0.12117267400026321,\n",
    "        0.03401527553796768,\n",
    "        -0.04343205690383911\n",
    "      ],\n",
    "      [\n",
    "        0.12117267400026321,\n",
    "        -0.03401527553796768,\n",
    "        0.035369038581848145,\n",
    "        -0.12117267400026321,\n",
    "        0.03401527553796768,\n",
    "        -0.035369038581848145\n",
    "      ],\n",
    "      [\n",
    "        0.1717672348022461,\n",
    "        -0.03401527553796768,\n",
    "        0.10869491845369339,\n",
    "        -0.1717672348022461,\n",
    "        0.03401527553796768,\n",
    "        -0.10869491845369339\n",
    "      ],\n",
    "      [\n",
    "        0.2106027454137802,\n",
    "        -0.03401527553796768,\n",
    "        0.09224789589643478,\n",
    "        -0.2106027454137802,\n",
    "        0.03401527553796768,\n",
    "        -0.09224789589643478\n",
    "      ],\n",
    "      [\n",
    "        0.28900790214538574,\n",
    "        -0.03401527553796768,\n",
    "        0.16660094261169434,\n",
    "        -0.28900790214538574,\n",
    "        0.03401527553796768,\n",
    "        -0.16660094261169434\n",
    "      ],\n",
    "      [\n",
    "        0.28900790214538574,\n",
    "        -0.03401527553796768,\n",
    "        0.17652182281017303,\n",
    "        -0.28900790214538574,\n",
    "        0.03401527553796768,\n",
    "        -0.17652182281017303\n",
    "      ],\n",
    "      [\n",
    "        0.39697834849357605,\n",
    "        -0.03401527553796768,\n",
    "        0.22029921412467957,\n",
    "        -0.39697834849357605,\n",
    "        0.03401527553796768,\n",
    "        -0.22029921412467957\n",
    "      ],\n",
    "      [\n",
    "        0.39697834849357605,\n",
    "        -0.03401527553796768,\n",
    "        0.3080121576786041,\n",
    "        -0.39697834849357605,\n",
    "        0.03401527553796768,\n",
    "        -0.3080121576786041\n",
    "      ],\n",
    "      [\n",
    "        0.400627464056015,\n",
    "        -0.03401527553796768,\n",
    "        0.3626677691936493,\n",
    "        -0.400627464056015,\n",
    "        0.03401527553796768,\n",
    "        -0.3626677691936493\n",
    "      ],\n",
    "      [\n",
    "        0.40157681703567505,\n",
    "        -0.03401527553796768,\n",
    "        0.40157681703567505,\n",
    "        -0.40157681703567505,\n",
    "        0.03401527553796768,\n",
    "        -0.40157681703567505\n",
    "      ]\n",
    "    ]\n",
    "  ],\n",
    "  \"last\": 21279.8753\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8614b753",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = np.array(data[\"X1\"], dtype=np.float32)\n",
    "X2 = np.array(data[\"X2\"], dtype=np.float32)\n",
    "\n",
    "        # Reshape and stack inputs\n",
    "num_rows, dim1, dim2 = X2.shape\n",
    "X2 = X2.reshape(num_rows, dim1 * dim2)\n",
    "X111 = np.hstack((X1, X2))\n",
    "X111.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7195d243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01940c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val[len(X_val)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf8fec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af2a71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c411886f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2af684d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab80821",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecca03a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddfd61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract best parameters\n",
    "best_params = bo.max['params']\n",
    "\n",
    "# Create `hidden_units` and `dropout_rates` lists dynamically\n",
    "num_hidden_layers = int(best_params['num_hidden_layers'])\n",
    "hidden_units = [\n",
    "    int(best_params[f\"hidden_units_{i+1}\"]) for i in range(num_hidden_layers)\n",
    "]\n",
    "dropout_rates = [\n",
    "    best_params[f\"dropout_rate_{i+1}\"] for i in range(num_hidden_layers)\n",
    "]\n",
    "\n",
    "# Save the full structure, including metadata\n",
    "model_structure = {\n",
    "    \"input_size\": num_features,\n",
    "    \"num_classes\": num_classes,\n",
    "    \"num_hidden_layers\": num_hidden_layers,\n",
    "    \"hidden_units\": hidden_units,\n",
    "    \"dropout_rates\": dropout_rates,\n",
    "    \"lr\": best_params[\"lr\"],\n",
    "    \"weight_decay\": best_params[\"weight_decay\"]\n",
    "}\n",
    "\n",
    "with open(\"model_structure.json\", \"w\") as f:\n",
    "    json.dump(model_structure, f, indent=4)\n",
    "\n",
    "print(\"Model structure saved to model_structure.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb3201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {count_trainable_parameters(model)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7242ae90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scripted = torch.jit.script(model) # Export to TorchScript\n",
    "model_scripted.save('best_model_weights.pt') # Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e807032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172c7b65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba53eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4cb935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_hourly_data_with_price_development(intradayndx_agg, input_date):\n",
    "    # Filter the DataFrame for the given date\n",
    "    df_intra_day = intradayndx_agg[intradayndx_agg[\"Date\"] == input_date]\n",
    "\n",
    "    # Initialize dictionary to store hourly data and price developments\n",
    "    hourly_data = {}\n",
    "\n",
    "    # Define trading hours from 9:30 to 15:30 (removing the last half-hour)\n",
    "    trading_hours = [(9, 30), (10, 30), (11, 30), (12, 30), (13, 30), (14, 30), (15, 30)]\n",
    "\n",
    "    # Check if the day's last timestamp is at least 15:30 or later\n",
    "    if not df_intra_day.empty and df_intra_day[\"datetime\"].max().time() >= pd.to_datetime(\"15:30\").time():\n",
    "        # Iterate through each time period\n",
    "        for i in range(len(trading_hours) - 1):\n",
    "            start_hour, start_min = trading_hours[i]\n",
    "            end_hour, end_min = trading_hours[i + 1]\n",
    "\n",
    "            # Create key for this hour period\n",
    "            period_key = f\"{start_hour}:{start_min:02d}-{end_hour}:{end_min:02d}\"\n",
    "\n",
    "            # Filter data for this hour\n",
    "            mask = (df_intra_day[\"datetime\"].dt.hour == start_hour) & \\\n",
    "                   (df_intra_day[\"datetime\"].dt.minute >= start_min) & \\\n",
    "                   ((df_intra_day[\"datetime\"].dt.hour < end_hour) | \\\n",
    "                    ((df_intra_day[\"datetime\"].dt.hour == end_hour) & \\\n",
    "                     (df_intra_day[\"datetime\"].dt.minute < end_min)))\n",
    "            df_hour = df_intra_day[mask]\n",
    "\n",
    "            # If there's no data for this hour, skip\n",
    "            if df_hour.empty:\n",
    "                continue\n",
    "\n",
    "            # Extract open price of the first candle of the hour\n",
    "            open_day = df_hour.iloc[0][\"Open\"]\n",
    "\n",
    "            # Define knockout levels\n",
    "            knockout_long = 0.96 * open_day\n",
    "            knockout_short = 1.04 * open_day\n",
    "\n",
    "            # Calculate initial open price for both long and short options\n",
    "            open_price_long = pricelong(open_day, knockout_long)\n",
    "            open_price_short = priceshort(open_day, knockout_short)\n",
    "\n",
    "            # Initialize list to store price development for this hour\n",
    "            price_development_list = []\n",
    "\n",
    "            # Iterate through each row (candle) in the hour data\n",
    "            for _, row in df_hour.iterrows():\n",
    "                # Calculate high, low, close for both long and short options\n",
    "                high_long = pricelong(row[\"High\"], knockout_long)\n",
    "                low_long = pricelong(row[\"Low\"], knockout_long)\n",
    "                close_long = pricelong(row[\"Close\"], knockout_long)\n",
    "\n",
    "                high_short = priceshort(row[\"High\"], knockout_short)\n",
    "                low_short = priceshort(row[\"Low\"], knockout_short)\n",
    "                close_short = priceshort(row[\"Close\"], knockout_short)\n",
    "\n",
    "                # Calculate percentage changes relative to open price\n",
    "                perc_high_long = (high_long - open_price_long) / open_price_long\n",
    "                perc_low_long = (low_long - open_price_long) / open_price_long\n",
    "                perc_close_long = (close_long - open_price_long) / open_price_long\n",
    "\n",
    "                perc_high_short = (high_short - open_price_short) / open_price_short\n",
    "                perc_low_short = (low_short - open_price_short) / open_price_short\n",
    "                perc_close_short = (close_short - open_price_short) / open_price_short\n",
    "\n",
    "                # Append the calculated percentage changes to the list\n",
    "                price_development_list.append([\n",
    "                    perc_high_long, perc_low_long, perc_close_long,\n",
    "                    perc_high_short, perc_low_short, perc_close_short\n",
    "                ])\n",
    "\n",
    "            # Store both the hourly data and price developments in the dictionary\n",
    "            hourly_data[period_key] = {\n",
    "                \"data\": df_hour,\n",
    "                \"price_development\": price_development_list\n",
    "            }\n",
    "\n",
    "    # Return the hourly data dictionary with price developments\n",
    "    return hourly_data\n",
    "\n",
    "# Example usage\n",
    "input_date = \"2024-11-08\"\n",
    "hourly_data_result = get_hourly_data_with_price_development(intradayndx_agg, input_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fad8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_data_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6511d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
